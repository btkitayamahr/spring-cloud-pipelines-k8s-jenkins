// Do not edit this file (e.g. go instead to src/main/asciidoc)

:branch: master
image::https://badges.gitter.im/Join%20Chat.svg[Gitter, link="https://gitter.im/spring-cloud/spring-cloud-pipelines?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge"]
image::https://circleci.com/gh/spring-cloud/spring-cloud-pipelines.svg?style=svg["CircleCI", link="https://circleci.com/gh/spring-cloud/spring-cloud-pipelines"]
:jenkins-root-docs: https://raw.githubusercontent.com/spring-cloud/spring-cloud-pipelines/{branch}/docs-sources/src/main/asciidoc/images/jenkins
:concourse-root-docs: https://raw.githubusercontent.com/spring-cloud/spring-cloud-pipelines/{branch}/docs-sources/src/main/asciidoc/images/concourse
:intro-root-docs: https://raw.githubusercontent.com/spring-cloud/spring-cloud-pipelines/{branch}/docs-sources/src/main/asciidoc/images/intro
:demo-root-docs: https://raw.githubusercontent.com/spring-cloud/spring-cloud-pipelines/{branch}/docs-sources/src/main/asciidoc/images/demo
:cf-migration-root-docs: https://raw.githubusercontent.com/spring-cloud/spring-cloud-pipelines/{branch}/docs-sources/src/main/asciidoc/images/cf-migration

= Spring Cloud Pipelines

Spring, Spring Boot, and Spring Cloud are tools that let developers speed up the
time of creating new business features. It is common knowledge, however, that the
feature is only valuable if it is in production. That is why companies
spend a lot of time and resources on building their own deployment pipelines.

This project tries to solve the following problems:

* Creation of a common deployment pipeline.
* Propagation of good testing and deployment practices.
* Reducing the time required to deploy a feature to production.

A common way of running, configuring, and deploying applications lowers support costs
and time needed by new developers to blend in when they change projects.

== Introduction

This section describes the rationale
behind the opinionated pipeline. We go through each deployment
step and describe it in detail.

IMPORTANT: You do not need to use all the pieces of Spring Cloud Pipelines. You
can (and should) gradually migrate your applications to use those pieces of
Spring Cloud Pipelines that you think best suit your needs.

=== Five-second Introduction

Spring Cloud Pipelines provides scripts, configuration, and convention for automated
deployment pipeline creation for Jenkins and Concourse with Cloud Foundry or Kubernetes.
We support JVM languages, PHP, and NodeJS. Since SC-Pipelines uses bash scripts,
you can use it with whatever automation server you have.

=== Five-minute Introduction

Spring Cloud Pipelines comes with bash scripts (available under `common/src/main/bash`)
that represent the logic of all steps in our opinionated deployment pipeline.
Since we believe in convention over configuration, for the supported framework and
languages, we assume that the projects follow certain conventions of task naming,
profile setting, and so on. That way, if you create a new application,
your application can follow those conventions and the deployment pipeline works.
Since no one pipeline can serve the purposes of all
teams in a company, we believe that minor deployment pipeline tweaking should take place.
That is why we allow the usage of that `sc-pipelines.yml` descriptor, which allows for
provide some customization.

From the pipeline visualization perspective, we have prepared templates for Concourse
and Jenkins (through the Jenkins Job DSL and Jenkinsfile). That means you can reuse them
immediately to visualize a deployment pipeline. If you use some other tool for
continuous delivery, you can set the visualization yourself and reference the
bash scripts for each step. In other words, Spring Cloud Pipelines can be reused
with any continuous delivery tool.

==== How to Use It

This repository can be treated as a template for your pipeline. We provide some opinionated
implementation that you can alter to suit your needs. To use it, we recommend downloading
the Spring Cloud Pipelines repository as a zip file, unzipping it in a directory,
initializing a Git project in that directory, and then modifying the project to suit your
needs. The following bash script shows how to do so:

====
[source,bash]
----
$ # pass the branch (e.g. master) or a particular tag (e.g. v1.0.0.RELEASE)
$ SC_PIPELINES_RELEASE=...
$ curl -LOk https://github.com/spring-cloud/spring-cloud-pipelines/archive/${SC_PIPELINES_RELEASE}.zip
$ unzip ${SC_PIPELINES_RELEASE}.zip
$ cd spring-cloud-pipelines-${SC_PIPELINES_RELEASE}
$ git init
$ # modify the pipelines to suit your needs
$ git add .
$ git commit -m "Initial commit"
$ git remote add origin ${YOUR_REPOSITORY_URL}
$ git push origin master
----
====

To keep your repository aligned with the changes in the upstream repository, you can also
clone the repository. To not have many merge conflicts, we recommend using the `custom`
folder hooks to override functions.

==== How It Works

As the following image shows, Spring Cloud Pipelines contains logic to generate a
pipeline and the runtime to execute pipeline steps.

image::{intro-root-docs}/how.png[title="How Spring Cloud Pipelines works"]

Once a pipeline is created (for example, by using the Jenkins Job DSL or from a Concourse
templated pipeline), when the jobs are ran, they clone or download Spring Cloud Pipelines
code to run each step. Those steps run functions that are
defined in the `commons` module of Spring Cloud Pipelines.

Spring Cloud Pipelines performs steps to guess what kind of a project your
repository is (for example, JVM or PHP) and what framework it uses (Maven or Gradle), and it
can deploy your application to a cloud (Cloud Foundry or Kubernetes). You can read about how
it works by reading the <<how-do-the-scripts-work>> section.

All of that happens automatically if your application follows the conventions.
You can read about them in the <<project-opinions>> section.

==== Supported Languages

Currently, we support the following languages:

* JVM
** Maven wrapper-based project
** Gradle wrapper-based project
* PHP
** Composer-based project
* NPM

==== Centralized Pipeline Creation

You can use Spring Cloud Pipelines to generate pipelines
for all the projects in your system. You can scan all your
repositories (for example, you can call the Stash or Github API to retrieve the list of repositories)
and then:

* For Jenkins, call the seed job and pass the `REPOS`
parameter, which contains the list of repositories.
* For Concourse, call `fly` and set the
pipeline for every repository.

TIP: We recommend using Spring Cloud Pipelines this way.

==== A Pipeline for Each Repository

You can use Spring Cloud Pipelines in such a way that
each project contains its own pipeline definition in
its code. Spring Cloud Pipelines clones the code with
the pipeline definitions (the bash scripts), so the
only piece of logic that needs to be in your application's
repository is the pipeline definition.

For Jenkins, you need to either set up the `Jenkinsfile`
or the jobs by using the Jenkins Job DSL plugin in your repo.
Then, in Jenkins, whenever you set up a new pipeline for a repository,
you can reference the pipeline definition in that repo.
For Concourse, each project contains its own pipeline steps,
and it is up to the project to set up the pipeline.

=== The Flow

The following images show the flow of the opinionated pipeline:

image::{intro-root-docs}/flow_concourse.png[title="Flow in Concourse"]

image::{intro-root-docs}/flow.png[title="Flow in Jenkins"]

We first describe the overall concept behind the flow and then
split it into pieces and describe each piece independently.

===Vocabulary

This section defines some common vocabulary. We describe four typical
environments in terms of running the pipeline.

==== Environments

We typically encounter the following environments:

* *build* environment is a machine where the building of the application takes place.
It is a continuous integration or continuous delivery tool worker.
* *test* is an environment where you can deploy an application to test it. It does not
resemble production, because we cannot be sure of its state (which application is deployed
there and in which version). It can be used by multiple teams at the same time.
* *stage* is an environment that does resemble production. Most likely, applications
are deployed there in versions that correspond to those deployed to production.
Typically, staging databases hold (often obfuscated) production data. Most
often, this environment is a single environment shared between many teams. In other
words, in order to run some performance and user acceptance tests, you have to block
and wait until the environment is free.
* *prod* is the production environment where we want our tested applications to be
deployed for our customers.

==== Tests

We typically encounter the following kinds of tests:

* *Unit tests*: Tests that run on the application during the build phase.
No integrations with databases or HTTP server stubs or other resources take place.
Generally speaking, your application should have plenty of these tests to provide fast
feedback about whether your features work.

* *Integration tests*: Tests that run on the built application during the build phase.
Integrations with in-memory databases and HTTP server stubs take place. According to the
https://martinfowler.com/bliki/TestPyramid.html[test pyramid], in most cases, you should
not have many of these kind of tests.

* *Smoke tests*: Tests that run on a deployed application. The concept of these tests
is to check that the crucial parts of your application are working properly. If you have 100 features
in your application but you gain the most money from five features, you could write smoke tests
for those five features. We are talking about smoke tests of an application, not of
the whole system. In our understanding inside the opinionated pipeline, these tests are
executed against an application that is surrounded with stubs.

* *End-to-end tests*: Tests that run on a system composed of multiple applications.
These tests ensure that the tested feature works when the whole system is set up.
Due to the fact that it takes a lot of time, effort, and resources to maintain such an environment
and that these tests are often unreliable (due to many different moving pieces, such as network,
database, and others), you should have a handful of those tests. They should be only for critical parts of your business.
Since only production is the key verifier of whether your feature works, some companies
do not even want to have these tests and move directly to deployment to production. When your
system contains KPI monitoring and alerting, you can quickly react when your deployed application
does not behave properly.

* *Performance testing*: Tests run on an application or set of applications
to check if your system can handle a big load. In the case of our opinionated pipeline,
these tests can run either on test (against a stubbed environment) or on
staging (against the whole system).

==== Testing against Stubs

Before we go into the details of the flow, consider the example described by the following image:

image::{intro-root-docs}/monolith.png[title="Two monolithic applications deployed for end to end testing"]

When you have only a handful of applications, end-to-end testing is beneficial.
From the operations perspective, it is maintainable for a finite number of deployed instances.
From the developers perspective, it is nice to verify the whole flow in the system
for a feature.

In the case of microservices, the scale starts to be a problem, as the following image shows:

image::{intro-root-docs}/many_microservices.png[title="Many microservices deployed in different versions"]

The following questions arise:

* Should I queue deployments of microservices on one testing environment or should I have an environment per microservice?
** If I queue deployments, people have to wait for hours to have their tests run. That can be a problem
* To remove that issue, I can have an environment for each microservice.
** Who will pay the bills? (Imagine 100 microservices, each having each own environment).
** Who will support each of those environments?
** Should we spawn a new environment each time we execute a new pipeline and then wrap it up or should we have
them up and running for the whole day?
* In which versions should I deploy the dependent microservices - development or production versions?
** If I have development versions, I can test my application against a feature that is not yet on production.
That can lead to exceptions in production.
** If I test against production versions, I can never test against a feature under development
anytime before deployment to production.

One of the possibilities of tackling these problems is to not do end-to-end tests.

The following image shows one solution to the problem, in the form of stubbed dependencies:

image::{intro-root-docs}/stubbed_dependencies.png[title="Execute tests on a deployed microservice on stubbed dependencies"]

If we stub out all the dependencies of our application, most of the problems presented earlier
disappear. There is no need to start and setup the infrastructure required by the dependent
microservices. That way, the testing setup looks like the following image:

image::{intro-root-docs}/stubbed_dependencies.png[title="We're testing microservices in isolation"]

Such an approach to testing and deployment gives the following benefits
(thanks to the usage of http://cloud.spring.io/spring-cloud-contract/spring-cloud-contract.html[Spring Cloud Contract]):

* No need to deploy dependent services.
* The stubs used for the tests run on a deployed microservice are the same as those used during integration tests.
* Those stubs have been tested against the application that produces them (see http://cloud.spring.io/spring-cloud-contract/spring-cloud-contract.html[Spring Cloud Contract] for more information).
* We do not have many slow tests running on a deployed application, so the pipeline gets executed much faster.
* We do not have to queue deployments. We test in isolation so that pipelines do not interfere with each other.
* We do not have to spawn virtual machines each time for deployment purposes.

However, this approach brings the following challenges:

* No end-to-end tests before production. You do not have full certainty that a feature is working.
* The first time the applications interact in a real way is on production.

As with every solution, it has its benefits and drawbacks. The opinionated pipeline
lets you configure whether you want to follow this flow or not.

==== General View

The general view behind this deployment pipeline is to:

* Test the application in isolation.
* Test the backwards compatibility of the application, in order to roll it back if necessary.
* Allow testing of the packaged application in a deployed environment.
* Allow user acceptance tests and performance tests in a deployed environment.
* Allow deployment to production.

The pipeline could have been split to more steps, but it seems that all of the aforementioned
actions fit nicely in our opinionated proposal.

=== Pipeline Descriptor

Each application can contain a file (called `sc-pipelines.yml`) with the following structure:

====
[source,yaml]
----
language_type: jvm
pipeline:
	# used for multi module projects
	main_module: things/thing
	# used for multi projects
	project_names:
		- monoRepoA
		- monoRepoB
	# should deploy to stage automatically and run e2e tests
	auto_stage: true
	# should deploy to production automatically
	auto_prod: true
	# should the api compatibility check be there
	api_compatibility_step: true
	# should the test rollback step be there
	rollback_step: true
	# should the stage step be there
	stage_step: true
	# should the test step (including rollback) be there
	test_step: true
lowercaseEnvironmentName1:
	# used by spinnaker
	deployment_strategy: HIGHlANDER
	# list of services to be deployed
	services:
		- type: service1Type
		  name: service1Name
		  coordinates: value
		- type: service2Type
		  name: service2Name
		  key: value
lowercaseEnvironmentName2:
	# used by spinnaker
	deployment_strategy: HIGHlANDER
	# list of services to be deployed
	services:
		- type: service3Type
		  name: service3Name
		  coordinates: value
		- type: service4Type
		  name: service4Name
		  key: value
----
====

If you have a multi-module project, you should point to the folder that contains the
module that produces the fat jar. In the preceding example, that module
would be present under the `things/thing` folder. If you have a single module project,
you need not create this section.

For a given environment, we declare a list of infrastructure services that we
want to have deployed. Services have:

* `type` (examples: `eureka`, `mysql`, `rabbitmq`, and `stubrunner`): This value gets
then applied to the `deployService` Bash function
* *[KUBERNETES]*: For `mysql`, you can pass the database name in the `database` property.
* `name`: The name of the service to get deployed.
* `coordinates`: The coordinates that let you fetch the binary of the service.
It can be a Maven coordinate (`groupid:artifactid:version`),
a docker image (`organization/nameOfImage`), and so on.
* Arbitrary key value pairs, which let you customize the services as you wish.

==== Pipeline Descriptor for Cloud Foundry

When deploying to Cloud Foundry you can provide services
of the following types:

* `type: broker`
** `broker`: The name of the CF broker
** `plan`: The name of the plan
** `params`: Additional parameters are converted to JSON
** `useExisting`: Whether to use an existing one or
create a new one (defaults to `false`)
* `type: app`
** `coordinates`: The Maven coordinates of the stub runner jar
** `manifestPath`: The path to the manifest for the stub runner jar
* `type: cups`
** `params`: Additional parameters are converted to JSON
* `type: cupsSyslog`
** `url`: The URL to the syslog drain
* `type: cupsRoute`
** `url`: The URL to the route service
* `type: stubrunner`
** `coordinates`: The Maven coordinates of the stub runner jar
** `manifestPath`: The path to the manifest for the stub runner jar

The following example shows the contents of a YAML file that defines the preceding values:

====
[source,yaml]
----
# This file describes which services are required by this application
# in order for the smoke tests on the TEST environment and end to end tests
# on the STAGE environment to pass

# lowercase name of the environment
test:
  # list of required services
  services:
    - name: config-server
      type: broker
      broker: p-config-server
      plan: standard
      params:
        git:
          uri: https://github.com/ciberkleid/app-config
      useExisting: true
    - name: cloud-bus
      type: broker
      broker: cloudamqp
      plan: lemur
      useExisting: true
    - name: service-registry
      type: broker
      broker: p-service-registry
      plan: standard
      useExisting: true
    - name: circuit-breaker-dashboard
      type: broker
      broker: p-circuit-breaker-dashboard
      plan: standard
      useExisting: true
    - name: stubrunner
      type: stubrunner
      coordinates: io.pivotal:cloudfoundry-stub-runner-boot:0.0.1.M1
      manifestPath: sc-pipelines/manifest-stubrunner.yml

stage:
  services:
    - name: config-server
      type: broker
      broker: p-config-server
      plan: standard
      params:
        git:
          uri: https://github.com/ciberkleid/app-config
    - name: cloud-bus
      type: broker
      broker: cloudamqp
      plan: lemur
    - name: service-registry
      type: broker
      broker: p-service-registry
      plan: standard
    - name: circuit-breaker-dashboard
      type: broker
      broker: p-circuit-breaker-dashboard
      plan: standard
----
====

Another CF specific property is `artifact_type`. Its value can be either `binary` or `source`.
Certain languages (such as Java) require a binary to be uploaded, but others (such as PHP)
require you to push the sources. The default value is `binary`.

=== Project Setup

Spring Cloud Pipelines supports three main types of project setup:

* `Single Project`
* `Multi Module`
* `Multi Project` (also known as mono repo)

A `Single Project` is a project that contains a single module that gets
built and packaged into a single, executable artifact.

A `Multi Module` project is a project that contains multiple modules.
After building all modules, one gets packaged into a single, executable artifact.
You have to point to that module in your pipeline descriptor.

A `Multi Project` is a project that contains multiple projects. Each of those
projects can in turn be a `Single Project` or a `Multi Module` project. Spring
Cloud Pipelines assume that, if a `PROJECT_NAME` environment
variable corresponds to a folder with the same name in the root of the
repository, this is the project it should build. For example, for
`PROJECT_NAME=something`, if there's a folder named `something`, then Spring Cloud Pipelines
treats the `something` directory as the root of the `something` project.

[[how-do-the-scripts-work]]
== How the Scripts Work

This section describes how the scripts and jobs correspond to each other.
If you need to see detailed documentation of the bash scripts, go to the
code repository and read `common/src/main/bash/README.adoc`.

[[build-and-deployment]]
=== Build and Deployment

The following text image (created via https://textart.io/sequence[textart.io]) shows a high-level overview:

```
+---------+                      +-----------+                      +-----------+ +-------+ +---------------+
| script  |                      | language  |                      | framework | | paas  | | customization |
+---------+                      +-----------+                      +-----------+ +-------+ +---------------+
     |                                 |                                  |           |             |
     | What is your language?          |                                  |           |             |
     |-------------------------------->|                                  |           |             |
     |                                 |                                  |           |             |
     |       I'm written in X language |                                  |           |             |
     |<--------------------------------|                                  |           |             |
     |                                 |                                  |           |             |
     |                                 | What framework do you use?       |           |             |
     |                                 |--------------------------------->|           |             |
     |                                 |                                  |           |             |
     |                                 |                I use Y framework |           |             |
     |<-------------------------------------------------------------------|           |             |
     |                                 |                                  |           |             |
     | I know that you use Z PAAS?     |                                  |           |             |
     |------------------------------------------------------------------------------->|             |
     |                                 |                                  |           |             |
     |                                 |  Here are all Z-related deployment functions |             |
     |<-------------------------------------------------------------------------------|             |
     |                                 |                                  |           |             |
     | Anything custom to override in bash?                               |           |             |
     |--------------------------------------------------------------------------------------------->|
     |                                 |                                  |           |             |
     |                                 |                                  |        Not this time... |
     |<---------------------------------------------------------------------------------------------|
     |                                 |                                  |           |             |
     | Ok, run the script              |                                  |           |             |
     |-------------------              |                                  |           |             |
     |                  |              |                                  |           |             |
     |<------------------              |                                  |           |             |
     |                                 |                                  |           |             |
```

Before we run the script, we need to answer a few questions related to your repository:

* What is your language (for example, `jvm`,`php`, or something else)?
* what framework do you use (for example, `maven` or `gradle`)?
* what PAAS do you use (for example, `cf` or `k8s`)?


The following sequence diagram (created via https://textart.io/sequence[textart.io]) describes how the sourcing of bash scripts takes place:

```
+---------+                                         +-----------+                                            +-------------+                   +-----------+            +-----------+                                   +-------+                            +---------+
| script  |                                         | pipeline  |                                            | projectType |                   | language  |            | framework |                                   | paas  |                            | custom  |
+---------+                                         +-----------+                                            +-------------+                   +-----------+            +-----------+                                   +-------+                            +---------+
     |                                                    |                                                         |                                |                        |                                             |                                     |
     | [source pipeline.sh]                               |                                                         |                                |                        |                                             |                                     |
     |--------------------------------------------------->|                                                         |                                |                        |                                             |                                     |
     |                                                    | ------------------------------\                         |                                |                        |                                             |                                     |
     |                                                    |-| loading functions, env vars |                         |                                |                        |                                             |                                     |
     |                                                    | |-----------------------------|                         |                                |                        |                                             |                                     |
     |         -----------------------------------------\ |                                                         |                                |                        |                                             |                                     |
     |         | hopefully all functions get overridden |-|                                                         |                                |                        |                                             |                                     |
     |         | otherwise nothing will work            | |                                                         |                                |                        |                                             |                                     |
     |         |----------------------------------------| |                                                         |                                |                        |                                             |                                     |
     |                                                    | Source the [projectType/pipeline-projectType.sh]        |                                |                        |                                             |                                     |
     |                                                    |-------------------------------------------------------->|                                |                        |                                             |                                     |
     |                                                    |                        -------------------------------\ |                                |                        |                                             |                                     |
     |                                                    |                        | What do we have here...?     |-|                                |                        |                                             |                                     |
     |                                                    |                        | A [mvnw] file,               | |                                |                        |                                             |                                     |
     |                                                    |                        | it has to be a [jvm] project | |                                |                        |                                             |                                     |
     |                                                    |                        |------------------------------| | Source [pipeline-jvm.sh]       |                        |                                             |                                     |
     |                                                    |                                                         |------------------------------->|                        |                                             |                                     |
     |                                                    |                                                         |                                |                        |                                             |                                     |
     |                                                    |                                                         |                                | Maven or Gradle?       |                                             |                                     |
     |                                                    |                                                         |                                |----------------------->|                                             |                                     |
     |                                                    |                                                         |                                |                        | ----------------------------------------\   |                                     |
     |                                                    |                                                         |                                |                        |-| There's a [mvnw] file?                |   |                                     |
     |                                                    |                                                         |                                |                        | | So the [PROJECT_TYPE] must be [maven] |   |                                     |
     |                                                    |                                                         |                                |                        | |---------------------------------------|   |                                     |
     |                                                    |                                                         |                                |   It's a Maven project |                                             |                                     |
     |                                                    |<------------------------------------------------------------------------------------------------------------------|                                             |                                     |
     |                                                    |                                                         |                                |                        |                                             |                                     |
     |                                                    | The [PAAS_TYPE] is [cf] so I'll source [pipeline-cf.sh] |                                |                        |                                             |                                     |
     |                                                    |---------------------------------------------------------------------------------------------------------------------------------------------------------------->|                                     |
     |                                                    |                                                         |                                |                        |                                             | -------------------------------\    |
     |                                                    |                                                         |                                |                        |                                             |-| Loading all                  |    |
     |                                                    |                                                         |                                |                        |                                             | | deployment-related functions |    |
     |                   -------------------------------\ |                                                         |                                |                        |                                             | |------------------------------|    |
     |                   | Ok, we know that it's Maven  |-|                                                         |                                |                        |                                             |                                     |
     |                   | and should be deployed to CF | |                                                         |                                |                        |                                             |                                     |
     |                   |------------------------------| |                                                         |                                |                        |                                             |                                     |
     |                                                    | Try to source [custom/build_and_upload.sh]              |                                |                        |                                             |                                     |
     |                                                    |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------>|
     |                                                    |                                                         |                                |                        |                                             |                                     | ----------------------------\
     |                                                    |                                                         |                                |                        |                                             |                                     |-| No such file so           |
     |                                                    |                                                         |                                |                        |                                             |                                     | | nothing custom to be done |
     | ---------------------------------------------\     |                                                         |                                |                        |                                             |                                     | |---------------------------|
     |-| All build related functions                |     |                                                         |                                |                        |                                             |                                     |
     | | overridden by language / framework scripts |     |                                                         |                                |                        |                                             |                                     |
     | -------------------------------\-------------|     |                                                         |                                |                        |                                             |                                     |
     |-| All deploy related functions |                   |                                                         |                                |                        |                                             |                                     |
     | | overridden by paas scripts   |                   |                                                         |                                |                        |                                             |                                     |
     | |------------------------------|                   |                                                         |                                |                        |                                             |                                     |
     | run [build] function                               |                                                         |                                |                        |                                             |                                     |
     |---------------------                               |                                                         |                                |                        |                                             |                                     |
     |                    |                               |                                                         |                                |                        |                                             |                                     |
     |<--------------------                               |                                                         |                                |                        |                                             |                                     |
     |                                                    |                                                         |                                |                        |                                             |                                     |
```

The process works as follows:

. A script (for example, `build_and_upload.sh`) is called.
. It sources the `pipeline.sh` script that contains all the essential function "`interfaces`" and
environment variables.
. `pipeline.sh` needs information about the project type. It
sources `projectType/pipeline-projectType.sh`.
. `projectType/pipeline-projectType.sh` contains logic to determine the language.
.. Verify whether a repository contains files that correspond to the given languages (for example, `mvnw` or `composer.json`).
.. Verify whether a concrete framework that we support (for example, `maven` or `gradle`) is present.
. Once we know what the project type is, we can deal with PAAS. Depending on the value of the `PAAS_TYPE` environment
variable, we can source proper PAAS functions (for example, `pipeline-cf.sh` for Cloud Foundry).
. Determine whether we can do some further customization.
.. Search for a file called `${sc-pipelines-root}/common/src/main/bash/custom/build_and_upload.sh`
to override any functions you want.
. Run the `build` function from `build_and_upload.sh`

[[project-crawler]]
=== Project Crawler

In Jenkins, you can generate the deployment pipelines by passing an environment variable
with a comma-separated list of repositories. This, however, does not scale. We would like to automatically fetch
a list of all repositories from a given organization and team.

To do so, we use the https://github.com/spring-cloud/project-crawler[Project Crawler]
library, which can:

* Fetch all projects for a given organization.
* Fetch contents of a file for a given repository.

The following diagram depicts this situation:

```
+---------+                                                  +-------+                                                                           +-------------+ +---------+
| Jenkins |                                                  | Seed  |                                                                           | SCPipelines | | Github  |
+---------+                                                  +-------+                                                                           +-------------+ +---------+
     |                                                           |                                                                                      |             |
     | Copy the seed job from the repo                           |                                                                                      |             |
     |------------------------------------------------------------------------------------------------------------------------------------------------->|             |
     |                                                           |                                                                                      |             |
     | Run seed job to generate Spinnaker pipelines and jobs     |                                                                                      |             |
     |---------------------------------------------------------->|                                                                                      |             |
     |                                                           |                                                                                      |             |
     |                                                           | Crawl org [foo] and fetch all repositories                                           |             |
     |                                                           |--------------------------------------------------------------------------------------------------->|
     |                                                           |                                                                                      |             |
     |                                                           |                                                                   In org [foo] there [a,b,c] repos |
     |                                                           |<---------------------------------------------------------------------------------------------------|
     |                                                           |                                                                                      |             |
     |                                                           | For each repo fetch pipeline descriptor                                              |             |
     |                                                           |--------------------------------------------------------------------------------------------------->|
     |                                                           |                                                                                      |             |
     |                                                           |                      There you go. [a] wants no [test] env, [b] no [stage] env, [c] wants all envs |
     |                                                           |<---------------------------------------------------------------------------------------------------|
     |                                                           |                                                                                      |             |
     |                                                           | Build pipelines. For [a] without [test], for [b] without [stage]. All for [c]        |             |
     |                                                           |------------------------------------------------------------------------------        |             |
     |                                                           |                                                                             |        |             |
     |                                                           |<-----------------------------------------------------------------------------        |             |
     |                             ----------------------------\ |                                                                                      |             |
     |                             | By having descriptors,    |-|                                                                                      |             |
     |                             | we can tune the pipelines | |                                                                                      |             |
     |                             | as the app wanted it to.  | |                                                                                      |             |
     |                             |---------------------------| | Build jobs / pipelines for [a,b,c] repos                                             |             |
     |                                                           |-----------------------------------------                                             |             |
     |                                                           |                                        |                                             |             |
     |                                                           |<----------------------------------------                                             |             |
     |                                                           |                                                                                      |             |
```

Thanks to the Project Crawler, you can run the seed job, and ,automatically, all the new repositories
are picked and pipelines are created for them. Project Crawler supports repositories
stored at Github, Gitlab, and Bitbucket. You can also register your own implementation. See the
https://github.com/spring-cloud/project-crawler[Project Crawler] repository for more information.

[[how-do-the-scripts-work-with-spinanker]]
=== How Scripts Work with Spinnaker

With Spinnaker, the deployment pipeline is inside of Spinnaker. No longer do we treat
Jenkins or Concourse as a tool that does deployments. In Jenkins, we create only
the CI jobs (that is, build and test) and prepare the JSON definitions of Spinnaker pipelines.

The following diagram shows how Jenkins, the seed job for Spinnaker, and Spinnaker cooperate:

```
+---------+                                                  +-------+                                                                           +-------------+                          +---------+ +-----------+
| Jenkins |                                                  | Seed  |                                                                           | SCPipelines |                          | Github  | | Spinnaker |
+---------+                                                  +-------+                                                                           +-------------+                          +---------+ +-----------+
     |                                                           |                                                                                      |                                      |            |
     | Copy the seed job from the repo                           |                                                                                      |                                      |            |
     |------------------------------------------------------------------------------------------------------------------------------------------------->|                                      |            |
     |                                                           |                                                                                      |                                      |            |
     | Run seed job to generate Spinnaker pipelines and jobs     |                                                                                      |                                      |            |
     |---------------------------------------------------------->|                                                                                      |                                      |            |
     |                                                           |                                                                                      |                                      |            |
     |                                                           | Crawl org [foo] and fetch all repositories                                           |                                      |            |
     |                                                           |---------------------------------------------------------------------------------------------------------------------------->|            |
     |                                                           |                                                                                      |                                      |            |
     |                                                           |                                                                                      |     In org [foo] there [a,b,c] repos |            |
     |                                                           |<----------------------------------------------------------------------------------------------------------------------------|            |
     |                                                           |                                                                                      |                                      |            |
     |                                                           | For each repo fetch pipeline descriptor                                              |                                      |            |
     |                                                           |---------------------------------------------------------------------------------------------------------------------------->|            |
     |                                                           |                                                                                      |                                      |            |
     |                                                           |                                                            There you go. [a] wants no [test], [b] no [stage], [c] wants all |            |
     |                                                           |<----------------------------------------------------------------------------------------------------------------------------|            |
     |                                                           |                                                                                      |                                      |            |
     |                                                           | Build pipelines. For [a] without [test], for [b] without [stage]. All for [c]        |                                      |            |
     |                                                           |------------------------------------------------------------------------------        |                                      |            |
     |                                                           |                                                                             |        |                                      |            |
     |                                                           |<-----------------------------------------------------------------------------        |                                      |            |
     |                             ----------------------------\ |                                                                                      |                                      |            |
     |                             | By having descriptors,    |-|                                                                                      |                                      |            |
     |                             | we can tune the pipelines | |                                                                                      |                                      |            |
     |                             | as the app wanted it to.  | |                                                                                      |                                      |            |
     |                             |---------------------------| | Build CI jobs for [a,b,c] repos                                                      |                                      |            |
     |                                                           |--------------------------------                                                      |                                      |            |
     |                                                           |                               |                                                      |                                      |            |
     |                                                           |<-------------------------------                                                      |                                      |            |
     |                                                           |                                                                                      |                                      |            |
     |                                                           | Build Spinnaker pipelines JSON definitions                                           |                                      |            |
     |                                                           |-------------------------------------------                                           |                                      |            |
     |                                                           |                                          |                                           |                                      |            |
     |                                                           |<------------------------------------------                                           |                                      |            |
     |                                                           |                                                                                      |                                      |            |
     |                                             Seed job done |                                                                                      |                                      |            |
     |<----------------------------------------------------------|                                                                                      |                                      |            |
     |                                                           |                                                                                      |                                      |            |
     | Upload JSON pipelines to Spinnaker                        |                                                                                      |                                      |            |
     |----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->|
     |                                                           |                                                                                      |                                      |            |
     |                                                           |                                                                                      |                                      |            | The pipelines for [a,b,c] successfully created
     |                                                           |                                                                                      |                                      |            |-----------------------------------------------
     |                                                           |                                                                                      |                                      |            |                                              |
     |                                                           |                                                                                      |                                      |            |<----------------------------------------------
     |                                                           |                                                                                      |                                      |            |
     |                                                           |                                                                                Waiting for [spinnaker-a-build] build to start & complete |
     |<-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
     |                                                           |                                                                                      |                                      |            |
     | New commit! Running a build [spinnaker-a-build]           |                                                                                      |                                      |            |
     |------------------------------------------------           |                                                                                      |                                      |            |
     |                                               |           |                                                                                      |                                      |            |
     |<-----------------------------------------------           |                                                                                      |                                      |            |
     |                                                           |                                                                                      |                                      |            |
     | Run the [build_and_upload.sh] script                      |                                                                                      |                                      |            |
     |------------------------------------------------------------------------------------------------------------------------------------------------->|                                      |            |
     |                                                           |                                                                                      | --------------------------------\    |            |
     |                                                           |                                                                                      |-| Proceed with all the sourcing |    |            |
     |                                                           |                                                                                      | | depending on language etc.    |    |            |
     |                                                           |                                                                                      | |-------------------------------|    |            |
     |                                                           |                                                                     Build completed! |                                      |            |
     |<-------------------------------------------------------------------------------------------------------------------------------------------------|                                      |            |
     |                                                           |                                                                                      |                                      |            |
     | [spinnaker-a-build] started and completed                 |                                                                                      |                                      |            |
     |----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->|
     |                                                           |                                                                                      |                                      |            | ------------------------------------\
     |                                                           |                                                                                      |                                      |            |-| Running the rest of the pipeline! |
     |                                                           |                                                                                      |                                      |            | |-----------------------------------|
     |                                                           |                                                                                      |                                      |            |
     |                                                           |                                                                                      |                                      |            | Pipeline for [a] in progress. Deploy [a] to test env
     |                                                           |                                                                                      |                                      |            |-----------------------------------------------------
     |                                                           |                                                                                      |                                      |            |                                                    |
     |                                                           |                                                                                      |                                      |            |<----------------------------------------------------
     |                                                           |                                                                                      |                                      |            |
     |                                                           |                                                                                   Calling [spinnaker-a-test-on-test] to run test on test |
     |<-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
     |                                                           |                                                                                      |                                      |            |
     | [spinnaker-a-test-on-test] started and completed          |                                                                                      |                                      |            |
     |----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->|
     |                                                           |                                                                                      |                                      |            |
     |                                                           |                                                                                      |                                      |            | ... we continue like this throughout the pipeline ...
     |                                                           |                                                                                      |                                      |            |------------------------------------------------------
     |                                                           |                                                                                      |                                      |            |                                                     |
     |                                                           |                                                                                      |                                      |            |<-----------------------------------------------------
     |                                                           |                                                                                      |                                      |            |
     |                                                           |                                                                                      |                                      |            | ... and the pipeline is done
     |                                                           |                                                                                      |                                      |            |-----------------------------
     |                                                           |                                                                                      |                                      |            |                            |
     |                                                           |                                                                                      |                                      |            |<----------------------------
     |                                                           |                                                                                      |                                      |            |
```

[[deployment-languages-compatibility-matrix]]
=== Deployment & languages compatibility matrix

In the following table we present which language is supported by which deployment
mechanism.

.Deployment & languages compatibility matrix
|===
|Language | CF | K8S | Ansible

| JVM with Gradle
| ✅
| ✅
| ✅

| JVM with Maven
| ✅
| ✅
| ✅

| PHP with Composer
| ✅
| ✅
| ❌

| NodeJS with NPM
| ✅
| ✅
| ❌

| Dotnet core
| ✅
| ✅
| ❌

|===

TIP: For K8S, a deployment unit is a docker image so any language and framework
can be used.

== Opinionated Implementation

This section describes a full flow of the demo applications.

IMPORTANT: Your applications need not have the same dependencies (such as `Eureka`) as this demo.

For demo purposes, we provide Docker Compose setup with Artifactory, Concourse, and Jenkins tools.
Regardless of the CD application, for the pipeline to pass, you need one of the following:

* A Cloud Foundry instance (for example, https://run.pivotal.io/[Pivotal Web Services] or https://pivotal.io/pcf-dev[PCF Dev]).
* A Kubernetes cluster (for example, https://github.com/kubernetes/minikube[Minikube]).
* The infrastructure applications deployed to the JAR hosting application (for the demo, we provide Artifactory).
* `Eureka` for Service Discovery.
* `Stub Runner Boot` for running Spring Cloud Contract stubs.

TIP: In the demos, we show you how to first build the `github-webhook` project. That is because
the `github-analytics` needs the stubs of `github-webhook` to pass the tests. We also use
references to the `github-analytics` project, since it contains more interesting pieces as far as testing
is concerned.

=== Build

The following image shows the results of building the demo pipeline (which the rest of this chapter describes):

image::{intro-root-docs}/build.png[title="Build and upload artifacts"]

In this step, we  generate a version of the pipeline. Next, we
run unit, integration, and contract tests. Finally, we:

* Publish a fat jar of the application.
* Publish a Spring Cloud Contract jar containing stubs of the application.
* For Kubernetes, upload a Docker image of the application.

During this phase, we run a `Maven` build by using Maven Wrapper or a `Gradle` build by using Gradle Wrapper,
with unit and integration tests. We also tag the repository with `dev/${version}`. That way, in each
subsequent step of the pipeline, we can retrieve the tagged version. Also, we know
exactly which version of the pipeline corresponds to which Git hash.

Once the artifact is built, we run API compatibility check, as follows:

* We search for the latest production deployment.
* We retrieve the contracts that were used by that deployment.
* From the contracts, we generat API tests to see if the current implementation
is fulfilling the HTTP and messaging contracts that the current production deployment
has defined (we check backward compatibility of the API).

=== Test

The following image shows the result of doing smoke tests and rolling back:

image::{intro-root-docs}/test.png[title="Smoke test and rollback test on test environment"]

Here, we:

* Start a RabbitMQ service in PaaS.
* Deploying `Eureka` infrastructure application to PaaS.
* Download the fat jar from Nexus and upload it to PaaS. We want the application
to run in isolation (be surrounded by stubs).

TIP: Currently, due to port constraints in Cloud Foundry,
we cannot run multiple stubbed HTTP services in the cloud. To fix this issue, we run
the application with the `smoke` Spring profile, on which you can stub out all HTTP calls to return
a mocked response.

* If the application uses a database, it gets upgraded at this point by Flyway, Liquibase,
or any other migration tool once the application gets started.
* From the project's Maven or Gradle build, we extract the `stubrunner.ids` property that contains
all the `groupId:artifactId:version:classifier` notations of dependent projects for which
the stubs should be downloaded.
* We upload `Stub Runner Boot` and pass the extracted `stubrunner.ids` to it. That way,
we have a running application in Cloud Foundry that downloads all the necessary stubs
of our application.
* From the checked-out code, we run the tests available under the `smoke` profile. In the
case of the `GitHub Analytics` application, we trigger a message from the `GitHub Webhook`
application's stub and send the message by RabbitMQ to GitHub Analytics. Then we check whether the
message count has increased.
* Once the tests pass, we search for the last production release. Once the application
is deployed to production, we tag it with `prod/${version}`. If there is no such tag
(there was no production release), no rollback tests are run. If there was
a production release, the tests get executed.
* Assuming that there was a production release, we check out the code that corresponds to that
release (we check out the tag), download the appropriate artifact (either a JAR for Cloud Foundry
or a Docker image for Kubernetes), and we upload
it to PaaS.

IMPORTANT: The old artifact runs against the *NEW* version of the database.

We run the old `smoke` tests against the freshly deployed application, surrounded by stubs.
If those tests pass, we have a high probability that the application is backwards compatible.
* The default behavior is that, after all of those steps, the user can manually click to deploy the
application to a stage environment.

=== Stage

The following image shows the result of deploying to a stage environment:

image::{intro-root-docs}/stage.png[title="End to end tests on stage environment"]

Here, we:

* Start a RabbitMQ service in PaaS.
* Deploy `Eureka` infrastructure application to PaaS.
* Download the artifact (either a JAR for Cloud Foundry or a Docker image for Kubernetes)
upload it to PaaS.

Next, we have a manual step in which, from the checked-out code, we run the tests available under the `e2e` profile. In the
case of the `GitHub Analytics` application, we send an HTTP message to the GitHub Analytics endpoint. Then we check whether
the received message count has increased.

By default, this step is manual, because the stage environment is often shared between
teams and some preparations on databases and infrastructure have to take place before the tests can be run.
Ideally, these step should be fully automatic.

=== Prod

The following image shows the result of deploying to a production environment:

image::{intro-root-docs}/prod.png[title="Deployment to production"]

The step to deploy to production is manual. However, ideally, it should be automatic.

IMPORTANT: This step does deployment to production. On production, we assume
that you have the infrastructure running. That is why, before you run this step, you
must run a script that provisions the services on the production environment.
For `Cloud Foundry`, call `tools/cf-helper.sh setup-prod-infra`.
For Kubernetes, call `tools/k8s-helper.sh setup-prod-infra`.

Here, we:

* Tag the Git repo with `prod/${version}`.
* Download the application artifact (either a JAR for Cloud Foundry or a Docker image for Kubernetes).
* We do Blue Green deployment:
** For Cloud Foundry:
*** We rename the current instance of the application (for example, `myService` to `myService-venerable`).
*** We deploy the new instance of the app under the `fooService` name
*** Now, two instances of the same application are running on production.
** For Kubernetes:
*** We deploy a service with the name of the application (for example, `myService`)
*** We do a deployment with the name of the application with version suffix,with the name escaped
to fulfill the DNS name requirements (for example, `fooService-1-0-0-M1-123-456-VERSION`).
*** All deployments of the same application have the same label `name`, which is equal to the application name (for example, `myService`).
*** The service routes the traffic by basing on the `name` label selector.
*** Now two instances of the same application are running in production.
* In the `Complete switch over`, which is a manual step, we stop the old instance.
+
NOTE: Remember to run this step only after you have confirmed that both instances work.
+
* In the `Rollback`, which is a manual step,
** We route all the traffic to the old instance.
*** In CF, we do that by ensuring that blue is running and removing green.
*** In K8S, we do that by scaling the number of instances of green to 0.
** We remov the latest prod Git tag.

[[project-opinions]]
== Project Opinions

This section goes through the assumptions we made in the project
structure and project properties.

=== Cloud Foundry Project Opinions

We take the following opinionated decisions for a Cloud Foundry based project:

* The application is built by using the Maven or Gradle wrapper.
* The application is deployed to Cloud Foundry.
* Your application needs a `manifest.yml` Cloud Foundry descriptor.
* For the Maven (https://github.com/spring-cloud-samples/github-webhook[example project]), we assume:
** Usage of the Maven Wrapper.
** `settings.xml` is parametrized to pass the credentials to push code to Artifactory:
*** `M2_SETTINGS_REPO_ID` contains the server ID for Artifactory or Nexus deployment.
*** `M2_SETTINGS_REPO_USERNAME` contains the username for Artifactory or Nexus deployment.
*** `M2_SETTINGS_REPO_PASSWORD` contains the password for Artifactory or Nexus deployment.
** Artifacts are deployed by `./mvnw clean deploy`.
** We use the `stubrunner.ids` property to retrieve list of collaborators for which stubs should be downloaded.
** `repo.with.binaries` property (injected by the pipeline): Contains the URL to the repo containing binaries (for example, Artifactory).
** `distribution.management.release.id` property (injected by the pipeline): Contains the ID of the distribution management. It corresponds to server ID in `settings.xml`.
** `distribution.management.release.url` property (injected by the pipeline): Contains the URL of the repository that contains binaries (for example, Artifactory).
** Running API compatibility tests with the `apicompatibility` Maven profile.
** `latest.production.version` property (injected by the pipeline): Contains the latest production version for the repo (retrieved from Git tags).
** Running smoke tests on a deployed app with the `smoke` Maven profile.
** Running end to end tests on a deployed app with the `e2e` Maven profile.
* For Gradle (https://github.com/spring-cloud-samples/github-analytics[example project] check the `gradle/pipeline.gradle` file), we assume:
** Usage of the Gradlew Wrapper.
** A `deploy` task for artifact deployment.
** The `REPO_WITH_BINARIES_FOR_UPLOAD` environment variable (Injected by the pipeline) contains the URL to the repository that contains binaries (for example, Artifactory).
** The `M2_SETTINGS_REPO_USERNAME` environment variable contains the user name used to send the binary to the repository that contains binaries (for exampl,e Artifactory).
** The `M2_SETTINGS_REPO_PASSWORD` environment variable contains the password used to send the binary to the repository that contains binaries (for example, Artifactory).
** Running API compatibility tests with the `apiCompatibility` task.
** `latestProductionVersion` property (injected by the pipeline): Contains the latest production version for the repository (retrieved from Git tags).
** Running smoke tests on a deployed app with the `smoke` task.
** Running end to end tests on a deployed app with the `e2e` task.
** `groupId` task to retrieve the group ID.
** `artifactId` task to retrieve the artifact ID.
** `currentVersion` task to retrieve the current version.
** `stubIds` task to retrieve the list of collaborators for which stubs should be downloaded.
* For PHP (https://github.com/spring-cloud-samples/cf-php-example[example project]), we asssume:
** Usage of https://getcomposer.org/[Composer].
** `composer install` is called to fetch libraries.
** The whole application is compressed to `tar.gz` and uploaded to binary storage.
*** `REPO_WITH_BINARIES_FOR_UPLOAD` environment variable (injected by the pipeline): Contains the URL of the repository that contains binaries (for example, Artifactory)
*** The `M2_SETTINGS_REPO_USERNAME` environment variable contains the user name used to send the binary to the repo containing binaries (for example, Artifactory).
*** The `M2_SETTINGS_REPO_PASSWORD` environment variable contains the password used to send the binary to the repo containing binaries (for example, Artifactory).
** `group-id`: Composer task that echoes the group ID.
** `app-name`: Composer task that echoes application name.
** `stub-ids`: Composer task that echoes stub runner ids.
** `test-apicompatibility`: Composer task that is executed for api compatibility tests.
** `test-smoke`: Composer task that is executed for smoke testing (the `APPLICATION_URL` and `STUBRUNNER_URL` environment variables are available here to be used).
** `test-e2e`: Composer task that is executed for end-to-end testing (`APPLICATION_URL` env vars is available here to be used)
** `target` is assumed to be the output folder. Put it in `.gitignore`
* For NodeJS (https://github.com/spring-cloud-samples/spring-cloud-contract-nodejs/tree/sc-pipelines[example project]), we assume:
** Usage of https://www.npmjs.com/[npm]
** `npm install` is called to fetch libraries.
** `npm test` is called to run tests.
** `npm run group-id`: npm task that echoes the group ID.
** `npm run app-name`: npm task that echoes application name.
** `npm run stub-ids`: npm task that echoes stub runner IDs.
** `npm run test-apicompatibility`: npm task that is executed for api compatibility tests.
** `npm run test-smoke`: npm task that is executed for smoke testing.
** `npm run test-e2e`: npm task that is executed for end-to-end testing.
** `target` is assumed to be the output folder. Put it in `.gitignore`
* For .Net (https://github.com/spring-cloud-samples/AspNetCoreExample[example project]):
** Usage of https://www.microsoft.com/net/core[ASP.NET core]
** `dotnet build` is called to build the project.
** `dotnet msbuild /nologo /t:CFPUnitTests` is called to run unit tests.
** `dotnet msbuild /nologo /t:CFPIntegrationTests` is called to run integration tests.
** `dotnet msbuild /nologo /t:CFPPublish /p:Configuration=Release` is called to publish a
ZIP with a self-contained DLL, together with all manifests and deployment files.
** `dotnet msbuild /nologo /t:CFPGroupId` is the npm task that echos the group ID.
** `dotnet msbuild /nologo /t:CFPAppName` is the npm task that echos application name.
** `dotnet msbuild /nologo /t:CFPStubIds` is the npm task that echos stub runner IDs.
** `dotnet msbuild /nologo /t:CFPApiCompatibilityTest` is run for API compatibility tests.
** `dotnet msbuild /nologo /t:CFPSmokeTests` is executed for smoke testing.
** `dotnet msbuild /nologo /t:CFPE2eTests` is executed for end-to-end testing.
** `target` is assumed to be the output folder. Add it to `.gitignore`.

=== Kubernetes Project Opinions

We use the following opinionated decisions for a Cloud Foundry based project:

* The application is built by using the Maven or Gradle wrappers.
* The application is deployed to Kubernetes.
* The Java Docker image needs to allow passing of system properties through the `SYSTEM_PROPS` environment variable.
* For Maven (https://github.com/spring-cloud-samples/github-webhook-kubernetes[example project]), we assume:
** Usage of the Maven Wrapper.
** `settings.xml` is parametrized to pass the credentials to push code to Artifactory and Docker repositories:
*** `M2_SETTINGS_REPO_ID`: Server ID for Artifactory or Nexus deployment.
*** `M2_SETTINGS_REPO_USERNAME`: User name for Artifactory or Nexus deployment.
*** `M2_SETTINGS_REPO_PASSWORD`: Password for Artifactory or Nexus deployment.
*** `DOCKER_SERVER_ID`: Server ID for Docker image pushing.
*** `DOCKER_USERNAME`: User name for Docker image pushing.
*** `DOCKER_PASSWORD`: Password for Docker image pushing.
*** `DOCKER_EMAIL`: Email for Artifactory or Nexus deployment
** `DOCKER_REGISTRY_URL` environment variable: Contains (Overridable - defaults to DockerHub) URL of the Docker registry.
** `DOCKER_REGISTRY_ORGANIZATION` environment variable: Contains the organization where your Docker repository resides.
** Artifacts and Docker image deployment is done by using `./mvnw clean deploy`.
** `stubrunner.ids` property: To retrieve list of collaborators for which stubs should be downloaded.
** `repo.with.binaries` property (injected by the pipeline): Contains the URL to the repo containing binaries (for example, Artifactory).
** `distribution.management.release.id` property (injected by the pipeline): Contains the ID of the distribution management. Corresponds to the server ID in `settings.xml`
** `distribution.management.release.url` property (injected by the pipeline): Contains the URL or the repository that contains binaries (for example, Artifactory).
** `deployment.yml` contains the Kubernetes deployment descriptor.
** `service.yml` contains the Kubernetes service descriptor.
** running API compatibility tests with the `apicompatibility` Maven profile.
** `latest.production.version` property (injected by the pipeline): Contains the latest production version for the repository (retrieved from Git tags).
** Running smoke tests on a deployed app with the `smoke` Maven profile.
** Running end to end tests on a deployed app with the `e2e` Maven profile.
* For Gradle  (https://github.com/spring-cloud-samples/github-analytics-kubernetes[example project] check the `gradle/pipeline.gradle` file), we assume:
** Usage of the Gradlew Wrapper.
** `deploy` task for artifact deployment.
** `REPO_WITH_BINARIES_FOR_UPLOAD` env var (injected by the pipeline): Contains the URL to the repository that contains binaries (for example, Artifactory).
** `M2_SETTINGS_REPO_USERNAME` environment variable: User name used to send the binary to the repository that contains binaries (for example, Artifactory).
** `M2_SETTINGS_REPO_PASSWORD` environment variable: Password used to send the binary to the repository that contains binaries (for example, Artifactory).
** `DOCKER_REGISTRY_URL` environment variable: (Overridable - defaults to DockerHub) URL of the Docker registry.
** `DOCKER_USERNAME` environment variable: User name used to send the the Docker image.
** `DOCKER_PASSWORD` environment variable: Password used to send the the Docker image.
** `DOCKER_EMAIL` environment variable: Email used to send the the Docker image.
** `DOCKER_REGISTRY_ORGANIZATION` environment variable: Contains the organization where your Docker repo resides.
** `deployment.yml` contains the Kubernetes deployment descriptor.
** `service.yml` contains the Kubernetes service descriptor.
** Running API compatibility tests with the `apiCompatibility` task.
** `latestProductionVersion` property (injected by the pipeline): Contains the latest production version for the repositoryi (retrieved from Git tags).
** Running smoke tests on a deployed application with the `smoke` task.
** Running end to end tests on a deployed application with the `e2e` task.
** `groupId` task to retrieve group ID.
** `artifactId` task to retrieve artifact ID.
** `currentVersion` task to retrieve the current version.
** `stubIds` task to retrieve the list of collaborators for which stubs should be downloaded.

== Customizing the Project

Spring Cloud Pipelines offers a number of ways to customize a Pipelines project:

* <<customization-overriding-scripts>>
* <<customization-overriding-pipelines>>
* <<customization-picking-features>>

[[customization-overriding-scripts]]
=== Overriding Scripts

Since Spring Cloud Pipelines evolves, you may want to pull the most recent changes to your
Spring Cloud Pipelines fork. To not have merge conflicts, the best approach
to extending the functionality is to use a separate script with customizations.

When we execute a script that represents a step (for example, a script named `build_and_upload.sh`),
after we source all the deployment and build-specific scripts (such as `pipeline-cf.sh`
and `projectType/pipeline-jvm.sh` with `projectType/pipeline-gradle.sh`), we set
a hook that lets you customize the behavior. If the script that we run
is `common/src/main/bash/build_and_upload.sh`, we search for a script in the
Spring Cloud Pipelines repository under `common/src/main/bash/custom/build_and_upload.sh`,
and we source that script just before running any functions.

The following example shows such a customization:

.custom/build_and_upload.sh
====
[source,bash]
----
#!/bin/bash

function build() {
    echo "I am executing a custom build function"
}

export -f build
----
====

when the `build` function is called for our Gradle project, instead of
calling the Gradle build process, we echo the following text: `I am executing a custom build function`.

[[customization-overriding-pipelines]]
=== Overriding Pipelines

Currently, the best way to extend the Concourse and Jenkins Jenkinsfile pipelines is to make
a copy of the Concourse pipeline `yaml` files and the Jenkins seed and pipeline jobs.

==== Overriding Jenkins Job DSL pipelines

We provide an interface (called `org.springframework.cloud.pipelines.common.JobCustomizer`)
that lets you provide customization for:

* all jobs
* build jobs
* test jobs
* stage jobs
* prod jobs

We use the JDK's `java.util.ServiceLoader` mechanism to achieve extensibility.

You can write an implementation of that interface (for example, `com.example.MyJubCustomizer`)
and create a `META-INF/org.springframework.cloud.pipelines.common.JobCustomizer` file in which you put the
`com.example.MyJubCustomizer` line.

If you create a JAR with your class (for example `com.example:my-customizer:1.0.0`),
put it on the build classpath, as the following example shows:

====
[source,groovy]
----
dependencies {
    // ...
    libs "com.example:my-customizer:1.0.0"
    // ...
}
----
====

If you do not want to create a separate library, you can create an implementation in the
sources under `src/main/resources/META-INF`.

Regardless of what you chose, your implementation runs for each job. You can add notifications
or any other customizations of your choosing.

[[customization-picking-features]]
=== Picking Features

If you want to pick only pieces (for example you want only `Cloud Foundry` combined with
`Concourse`), you can run the following command:

====
[source,bash]
----
$ ./gradlew customize
----
====

A screen resembling the following appears:

====
[source,bash]
----
:customize
  ___          _              ___ _             _   ___ _           _ _
 / __|_ __ _ _(_)_ _  __ _   / __| |___ _  _ __| | | _ (_)_ __  ___| (_)_ _  ___ ___
 \__ \ '_ \ '_| | ' \/ _` | | (__| / _ \ || / _` | |  _/ | '_ \/ -_) | | ' \/ -_|_-<
 |___/ .__/_| |_|_||_\__, |  \___|_\___/\_,_\__,_| |_| |_| .__/\___|_|_|_||_\___/__/
     |_|             |___/                               |_|



Follow the instructions presented in the console or terminate the process to quit (ctrl + c)


=== PAAS TYPE ===
Which PAAS type do you want to use? Options: [CF, K8S, BOTH]
<-------------> 0% EXECUTING
> :customize
----
====

Now you need to answer a couple of questions. Depending on your choices, whole files and their pieces
get removed and updated accordingly. For example, if you choose the `CF` and `Concourse` options,
the `Kubernetes` and `Jenkins` configuration and folders and pieces of code in
the project get removed.



[[concourse-pipeline-k8s]]
== Concourse Pipeline (Kubernetes)

IMPORTANT: In this chapter, we assume that you deploy your application
to Kubernetes PaaS

[[concourse]]
The Spring Cloud Pipelines repository contains opinionated
Concourse pipeline definitions. Those jobs form an empty pipeline and an
opinionated sample pipeline that you can use in your company.

The following projects take part in the `microservice setup` for this demo:

* https://github.com/spring-cloud-samples/github-analytics-kubernetes[Github Analytics]: The application that has a REST endpoint and uses messaging -- part of our business application.
* https://github.com/spring-cloud-samples/github-webhook-kubernetes[Github Webhook]: Project that emits messages that are used by Github Analytics -- part of our business application.
* https://github.com/spring-cloud-samples/github-eureka[Eureka]: Simple Eureka Server. This is an infrastructure application.
* https://github.com/spring-cloud-samples/github-analytics-stub-runner-boot[Github Analytics Stub Runner Boot]: Stub Runner Boot server to be used for tests with Github Analytics and uses Eureka and Messaging. This is an infrastructure application.

[[step-by-step-k8s]]
=== Step-by-step

If you want only to run the demo as far as possible by using PCF Dev and Docker Compose, do the following:

. <<concourse-fork-k8s,Fork repos>>
. <<concourse-start-k8s,Start Concourse and Artifactory>>
. <<concourse-pipeline-fly-k8s,Setup the `fly` CLI >>
. <<concourse-pipeline-credentials-k8s,Setup your `credentials.yml` >>
. <<concourse-pipeline-build-k8s,Setup the pipeline >>
. <<concourse-pipeline-run-k8s,Run the `github-webhook` pipeline>>

[[fork-repos-k8s]]
==== Fork Repositories

[[concourse-fork-k8s]]
Four applications compose the pipeline:

* https://github.com/spring-cloud-samples/github-webhook-kubernetes[Github Webhook]
* https://github.com/spring-cloud-samples/github-analytics-kubernetes/[Github Analytics]
* https://github.com/spring-cloud-samples/github-eureka[Github Eureka]
* https://github.com/spring-cloud-samples/github-analytics-stub-runner-boot-classpath-stubs[Github Stub Runner Boot]

You need to fork only the following repositories, because only then can you tag and push the tag to the repository:

* https://github.com/spring-cloud-samples/github-webhook-kubernetes[Github Webhook]
* https://github.com/spring-cloud-samples/github-analytics-kubernetes/[Github Analytics]

[[concourse-start-k8s]]
=== Concourse in K8S (Kubernetes)

The simplest way to deploy Concourse to K8S is to use https://github.com/kubernetes/helm[Helm].
Once you have Helm installed and your `kubectl` is pointing to the
cluster, run the following command to install the Concourse cluster in your K8S cluster:

====
[source,bash]
----
$ helm install stable/concourse --name concourse
----
====

Once the script is done, you should see the following output

====
[source,bash]
----
1. Concourse can be accessed:

  * Within your cluster, at the following DNS name at port 8080:

    concourse-web.default.svc.cluster.local

  * From outside the cluster, run these commands in the same shell:

    export POD_NAME=$(kubectl get pods --namespace default -l "app=concourse-web" -o jsonpath="{.items[0].metadata.name}")
    echo "Visit http://127.0.0.1:8080 to use Concourse"
    kubectl port-forward --namespace default $POD_NAME 8080:8080

2. Login with the following credentials

  Username: concourse
  Password: concourse
----
====

Follow the steps and log in to Concourse under http://127.0.0.1:8080.

==== Deploying Artifactory to K8S

You can use Helm also to deploy Artifactory to K8S, as follows:

====
[source,bash]
----
$ helm install --name artifactory --set artifactory.image.repository=docker.bintray.io/jfrog/artifactory-oss stable/artifactory
----
====

After you run this command, you should see the following output:

====
[source,bash]
----
NOTES:
Congratulations. You have just deployed JFrog Artifactory Pro!

1. Get the Artifactory URL by running these commands:

   NOTE: It may take a few minutes for the LoadBalancer IP to be available.
         You can watch the status of the service by running 'kubectl get svc -w nginx'
   export SERVICE_IP=$(kubectl get svc --namespace default nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
   echo http://$SERVICE_IP/

2. Open Artifactory in your browser
   Default credential for Artifactory:
   user: admin
   password: password
----
====

Next, you need to set up the repositories.

First, access the Artifactory URL and log in with
a user name of `admin` and a password of `password`.

image::{concourse-root-docs}/artifactory_quick_setup.png[title="Click on Quick Setup"]

Then, click on Maven setup and click `Create`.

image::{concourse-root-docs}/artifactory_maven_repo.png[title="Create the `Maven` Repository"]

[[concourse-pipeline-fly-k8s]]
==== Setup the `fly` CLI

[[fly]] If you go to the Concourse website you should see something resembling the following:

image::{concourse-root-docs}/running_concourse.png[]

You can click one of the icons (depending on your OS) to download `fly`, which is the Concourse CLI. Once you download that (and maybe added it to your PATH, depending on your OS) you can run the following command:

====
[source,bash]
----
fly --version
----
====

If `fly` is properly installed, it should print out the version.

[[concourse-pipeline-credentials-k8s]]
==== Setup your `credentials.yml`

We made a sample credentials file called `credentials-sample-k8s.yml`
prepared for `k8s`. You can use it as a base for your `credentials.yml`.

To allow the Concourse worker's spawned container to connect to the
Kubernetes cluster, you must pass the CA contents and the
auth token.

To get the contents of CA for GCE, run the following command:

====
[source,bash]
----
$ kubectl get secret $(kubectl get secret | grep default-token | awk '{print $1}') -o jsonpath='{.data.ca\.crt}' | base64 --decode
----
====

To get the auth token, run the following command:

====
[source,bash]
----
$ kubectl get secret $(kubectl get secret | grep default-token | awk '{print $1}') -o jsonpath='{.data.token}' | base64 --decode
----
====

Set that value under `paas-test-client-token`, `paas-stage-client-token`, and `paas-prod-client-token`

[[concourse-pipeline-build-k8s]]
==== Build the pipeline

After running Concourse, you should get the following output in your terminal:

====
[source,bash]
----
$ export POD_NAME=$(kubectl get pods --namespace default -l "app=concourse-web" -o jsonpath="{.items[0].metadata.name}")
$ echo "Visit http://127.0.0.1:8080 to use Concourse"
$ kubectl port-forward --namespace default $POD_NAME 8080:8080
Visit http://127.0.0.1:8080 to use Concourse
----
====

Log in (for example, for Concourse running at `127.0.0.1` -- if you do not provide any value, `localhost` is assumed). If you run this script, it assumes that either `fly` is on your `PATH` or that it is in the same folder as the script:

====
[source,bash]
----
$ fly -t k8s login -c http://localhost:8080 -u concourse -p concourse
----
====

Next, run the following command to create the pipeline:

====
[source,bash]
----
$ ./set_pipeline.sh github-webhook k8s credentials-k8s.yml
----
====

[[concourse-pipeline-run-k8s]]
==== Run the `github-webhook` Pipeline

The following images show the various steps involved in runnig the `github-webhook` pipeline:

{nbsp}
{nbsp}

image::{concourse-root-docs}/concourse_login.png[caption="Step 1: ", title="Click `Login`"]

{nbsp}
{nbsp}

image::{concourse-root-docs}/concourse_team_main.png[caption="Step 2: ", title="Pick `main` team"]

{nbsp}
{nbsp}

image::{concourse-root-docs}/concourse_user_pass.png[caption="Step 3: ", title="Log in with `concourse` user and `concourse` password"]

{nbsp}
{nbsp}

image::{concourse-root-docs}/concourse_pipeline.png[caption="Step 4: ", title="Your screen should look more or less like this"]

{nbsp}
{nbsp}

image::{concourse-root-docs}/start_pipeline.png[caption="Step 5: ", title="Unpause the pipeline by clicking in the top lefr corner and then clicking the `play` button"]

{nbsp}
{nbsp}

image::{concourse-root-docs}/generate_version.png[caption="Step 6: ", title="Click 'generate-version'"]

{nbsp}
{nbsp}

image::{concourse-root-docs}/run_pipeline.png[caption="Step 7: ", title="Click `+` sign to start a new build"]

{nbsp}
{nbsp}

image::{concourse-root-docs}/concourse_pending.png[caption="Step 8: ", title="The job is pending"]

{nbsp}
{nbsp}

image::{concourse-root-docs}/job_running.png[caption="Step 9: ", title="Job is pending in the main screen"]

{nbsp}
{nbsp}

image::{concourse-root-docs}/running_pipeline.png[caption="Step 10: ", title="Job is running in the main screen"]

== Jenkins Pipeline (Common)

In this section we will present the common setup of Jenkins for any platform.
We will also provide answers to most frequently asked questions.

=== Project setup

[source,bash]
----
.
├── declarative-pipeline
│   └── Jenkinsfile-sample.groovy
├── jobs
│   ├── jenkins_pipeline_empty.groovy
│   ├── jenkins_pipeline_jenkinsfile_empty.groovy
│   ├── jenkins_pipeline_sample.groovy
│   └── jenkins_pipeline_sample_view.groovy
├── seed
│   ├── init.groovy
│   ├── jenkins_pipeline.groovy
│   ├── k8s
│   └── settings.xml
└── src
    ├── main
    └── test
----

In the `declarative-pipeline` you can find a definition of a `Jenkinsfile-sample.groovy` declarative
pipeline. It's used together with the Blueocean UI.

In the `jobs` folder you have all the seed jobs that will generate pipelines.

- `jenkins_pipeline_empty.groovy` - is a template of a pipeline with empty steps using the Jenkins Job DSL plugin
- `jenkins_pipeline_jenkinsfile_empty.groovy` - is a template of a pipeline with empty steps using the Pipeline plugin
- `jenkins_pipeline_sample.groovy` - is an opinionated implementation using the Jenkins Job DSL plugin
- `jenkins_pipeline_sample_view.groovy` - builds the views for the pipelines

In the `seed` folder you have the `init.groovy` file which is executed when Jenkins starts.
That way we can configure most of Jenkins options for you (adding credentials, JDK etc.).
`jenkins_pipeline.groovy` contains logic to build a seed job (that way you don't have to even click that
job - we generate it for you). Under the `k8s` folder there are all the configuration
files required for deployment to a Kubernetes cluster.

In the `src` folder you have production and test classes needed for you to build your own pipeline.
Currently we have tests only cause the whole logic resides in the `jenkins_pipeline_sample` file.

=== Optional customization steps

[[jenkins_optional]] All the steps below are not necessary to run the demo. They are needed only
when you want to do some custom changes.

[[deploying-infra]]
==== Deploying infra jars to a different location

It's enough to set the `ARTIFACTORY_URL` environmental variable before
executing `tools/deploy-infra.sh`. Example for deploying to Artifactory at IP `192.168.99.100`

[source,bash]
----
git clone https://github.com/spring-cloud/spring-cloud-pipelines
cd spring-cloud-pipelines/
ARTIFACTORY_URL="http://192.168.99.100:8081/artifactory/libs-release-local" ./tools/deploy-infra.sh
----

[[setup-settings-xml]]
==== Setup settings.xml for Maven deployment

TIP: If you want to use the default connection to the Docker version
of Artifactory you can skip this step

[[jenkins-settings]] So that `./mvnw deploy` works with Artifactory from Docker we're
already copying the missing `settings.xml` file for you. It looks more or less like this:

[source,xml]
----
<?xml version="1.0" encoding="UTF-8"?>
<settings>
	<servers>
		<server>
			<id>${M2_SETTINGS_REPO_ID}</id>
			<username>${M2_SETTINGS_REPO_USERNAME}</username>
			<password>${M2_SETTINGS_REPO_PASSWORD}</password>
		</server>
		<server>
			<id>${DOCKER_SERVER_ID}</id>
			<username>${DOCKER_USERNAME}</username>
			<password>${DOCKER_PASSWORD}</password>
			<configuration>
				<email>${DOCKER_EMAIL}</email>
			</configuration>
		</server>
	</servers>
</settings>
----

As you can see the file is parameterized. In Maven it's enough to pass
to `./mvnw` command the proper system property to override that value. For example to pass
a different docker email you'd have to call `./mvnw -DDOCKER_EMAIL=foo@bar.com` and the value
gets updated.

If you want to use your own version of Artifactory / Nexus you have to update
the file (it's in `seed/settings.xml`).

[[setup-jenkins-env-vars]]
==== Setup Jenkins env vars

[[jenkins_env]] If you want to only play around with the demo that we've prepared you have to set *ONE* variable which is the `REPOS` variable.
That variable needs to consists of comma separated list of URLs to repositories containing business apps. So you should pass your forked repos URLs.

You can do it in the following ways:

- globally via Jenkins global env vars (then when you run the seed that variable will be taken into consideration and proper pipelines will get built)
- modify the seed job parameters (you'll have to modify the seed job configuration and change the `REPOS` property)
- provide the repos parameter when running the seed job

For the sake of simplicity let's go with the *last* option.

IMPORTANT: If you're choosing the global envs, you *HAVE* to remove the other approach
(e.g. if you set the global env for `REPOS`, please remove that property in the
seed job

[[setup-seed-props]]
===== Seed properties

Click on the seed job and pick `Build with parameters`. Then as presented in the screen below (you'll have far more properties to set) just modify the `REPOS` property by providing the comma separated list of URLs to your forks. Whatever you set will be parsed by the seed job and passed to the generated Jenkins jobs.

TIP: This is very useful when the repos you want to build differ. E.g. use
different JDK. Then some seeds can set the `JDK_VERSION` param to one version
of Java installation and the others to another one.

Example screen:

image::{jenkins-root-docs}/seed.png[]

In the screenshot we could parametrize the `REPOS` and `REPO_WITH_BINARIES` params.

[[global-envs]]
===== Global envs

IMPORTANT: This section is presented only for informational purposes - for the sake of demo you can skip it

You can add env vars (go to configure Jenkins -> Global Properties) for the following
 properties (example with defaults for PCF Dev):

Example screen:

image::{jenkins-root-docs}/env_vars.png[]

[[git-email]]
==== Set Git email / user

Since our pipeline is setting the git user / name explicitly for the build step
 you'd have to go to `Configure` of the build step and modify the Git name / email.
 If you want to set it globally you'll have to remove the section from the build
 step and follow these steps to set it globally.

You can set Git email / user globally like this:

{nbsp}
{nbsp}

image::{jenkins-root-docs}/manage_jenkins.png[caption="Step 1: ", title="Click 'Manage Jenkins'"]

{nbsp}
{nbsp}

image::{jenkins-root-docs}/configure_system.png[caption="Step 2: ", title="Click 'Configure System'"]

{nbsp}
{nbsp}

image::{jenkins-root-docs}/git.png[caption="Step 3: ", title="Fill out Git user information"]

{nbsp}
{nbsp}


[[jenkins-credentials-github]]
===== Add Jenkins credentials for GitHub

[[jenkins-credentials]] The scripts will need to access the credential in order to tag the repo.

You have to set credentials with id: `git`.

Below you can find instructions on how to set a credential (e.g. for Cloud Foundry `cf-test` credential but
remember to provide the one with id `git`).

{nbsp}
{nbsp}

image::{jenkins-root-docs}/credentials_system.png[caption="Step 1: ", title="Click 'Credentials, System'"]

{nbsp}
{nbsp}

image::{jenkins-root-docs}/credentials_global.png[caption="Step 2: ", title="Click 'Global Credentials'"]

{nbsp}
{nbsp}

image::{jenkins-root-docs}/credentials_add.png[caption="Step 3: ", title="Click 'Add credentials'"]

{nbsp}
{nbsp}

image::{jenkins-root-docs}/credentials_example.png[caption="Step 4: ", title="Fill out the user / password and provide the `git` credential ID (in this example `cf-test`)"]

{nbsp}
{nbsp}

=== Testing Jenkins scripts

`./gradlew clean build`

WARNING: The ran test only checks if your scripts compile.

=== How to work with Jenkins Job DSL plugin

Check out the https://github.com/jenkinsci/job-dsl-plugin/wiki/Tutorial---Using-the-Jenkins-Job-DSL[tutorial].
Provide the link to this repository in your Jenkins installation.

WARNING: Remember that views can be overridden that's why the suggestion is to contain in one script all the logic needed to build a view
 for a single project (check out that `spring_cloud_views.groovy` is building all the `spring-cloud` views).

=== Docker Image

If you would like to run the pre-configured Jenkins image somewhere other than your local machine, we
have an image you can pull and use on https://hub.docker.com/r/springcloud/spring-cloud-pipeline-jenkins/[DockerHub].
The `latest` tag corresponds to the latest snapshot build.  You can also find tags
corresponding to stable releases that you can use as well.

IMPORTANT: The Jenkins docker image is setup for demo purposes. For example it has the following
system property `-Dpermissive-script-security.enabled=no_security` that disables script
security. *YOU SHOULD NOT USE IT ON PRODUCTION UNLESS YOU KNOW WHAT YOU'RE DOING*.


[[jenkins-pipeline-k8s]]
== Jenkins Pipeline (Kubernetes)

IMPORTANT: In this chapter, we assume that you deploy your application
to Kubernetes PaaS.

[[jenkins]]
The Spring Cloud Pipelines repository contains job definitions and the opinionated setup pipeline that uses https://wiki.jenkins-ci.org/display/JENKINS/Job+DSL+Plugin[Jenkins Job DSL plugin]. Those jobs form an empty pipeline and an opinionated sample pipeline that you can use in your company.

The following projects take part in the `microservice setup` for this demo.

* https://github.com/spring-cloud-samples/github-analytics-kubernetes[Github Analytics]: The app that has a REST endpoint and uses messaging -- part of our business application.
* https://github.com/spring-cloud-samples/github-webhook-kubernetes[Github Webhook]: Project that emits messages that are used by Github Analytics -- part of our business application.
* https://github.com/spring-cloud-samples/github-eureka[Eureka]: Simple Eureka Server. This is an infrastructure application.
* https://github.com/spring-cloud-samples/github-analytics-stub-runner-boot[Github Analytics Stub Runner Boot]: Stub Runner Boot server to be used for tests with Github Analytics ad uses Eureka and Messaging. This is an infrastructure application.

[[step-by-step-k8s]]
=== Step-by-step

This is a guide for a Jenkins Job DSL based pipeline.

If you want only to run the demo as far as possible by using PCF Dev and Docker Compose, do the following:

. <<jenkins-fork-k8s,Fork repos>>
. <<jenkins-start-k8s,Start Jenkins and Artifactory>>
. <<jenkins-deploy-k8s,Deploy infra to Artifactory>>
. <<jenkins-minikube-k8s,Start Minikube (if you don't want to use an existing one)>>
. <<jenkins-seed-k8s,Run the seed job>>
. <<jenkins-pipeline-k8s,Run the `github-webhook` pipeline>>

[[fork-repos-k8s]]
==== Fork Repositories

[[jenkins-fork-k8s]]
Four applications compose the pipeline

* https://github.com/spring-cloud-samples/github-webhook-kubernetes[Github Webhook]
* https://github.com/spring-cloud-samples/github-analytics-kubernetes/[Github Analytics]
* https://github.com/spring-cloud-samples/github-eureka[Github Eureka]
* https://github.com/spring-cloud-samples/github-analytics-stub-runner-boot-classpath-stubs[Github Stub Runner Boot]

You need to fork only the following repositories, because only then can you tag and push the tag to your repository:

* https://github.com/spring-cloud-samples/github-webhook-kubernetes[Github Webhook]
* https://github.com/spring-cloud-samples/github-analytics-kubernetes/[Github Analytics]

[[start-jenkins-k8s]]
==== Start Jenkins and Artifactory

[[jenkins-start-k8s]]
Jenkins and Artifactory can be ran locally. To do so, run the
`start.sh` script from this repo. The following listing shows the script:

====
[source,bash]
----
git clone https://github.com/spring-cloud/spring-cloud-pipelines
cd spring-cloud-pipelines/jenkins
./start.sh yourGitUsername yourGitPassword yourForkedGithubOrg yourDockerRegistryOrganization yourDockerRegistryUsername yourDockerRegistryPassword yourDockerRegistryEmail
----
====

Then Jenkins runs on port `8080`, and Artifactory runs on port `8081`.
The provided parameters are passed as environment variables to the Jenkins VM
and credentials are set. That way, you need not do
any manual work on the Jenkins side. In the preceding script, the third parameter
could be `yourForkedGithubOrg` or `yourGithubUsername`. Also the `REPOS` environment variable
contains your GitHub org in which you have the forked repositories.

Instead of the Git username and password parameters, you could pass `-key <path_to_private_key>`
if you prefer to use the key-based authentication with your Git repositories.

You need to pass the credentials for the Docker organization (by default, we
search for the Docker images at Docker Hub) so that the pipeline can
push images to your org.

[[deploy-infra-k8s]]
===== Deploy the Infra JARs to Artifactory

[[jenkins-deploy-k8s]]
When Artifactory is running, run the `tools/deploy-infra.sh` script from this repo.
The following listing shows the script:

====
[source,bash]
----
git clone https://github.com/spring-cloud/spring-cloud-pipelines
cd spring-cloud-pipelines/
./tools/deploy-infra-k8s.sh
----
====

As a result, both the `eureka` and `stub runner` repos are cloned, built, and
uploaded to Artifactory and their docker images are built.

IMPORTANT: Your local Docker process is reused by the Jenkins instance running
in Docker. That is why you do not have to push these images to Docker Hub. On the
other hand, if you run this sample in a remote Kubernetes cluster, the driver
is not shared by the Jenkins workers, so you can consider pushing these
Docker images to Docker Hub too.

[[jenkins-seed-k8s]]
==== Run the seed job

We created the seed job for you, but you have to run it. When you do
run it, you have to provide some properties. By default we create a seed that
has all the properties options, but you can delete most of it. If you
set the properties as global environment variables, you have to remove them from the
seed.

To run the demo, provide a comma-separated
list of the URLs of the two aforementioned forks (`github-webhook` and `github-analytics') in the `REPOS` variable.

The following images shows the steps involved:

{nbsp}
{nbsp}

image::{jenkins-root-docs}/seed_click.png[caption="Step 1: ", title="Click the 'jenkins-pipeline-seed-cf' job for Cloud Foundry and `jenkins-pipeline-seed-k8s` for Kubernetes"]

{nbsp}
{nbsp}

image::{jenkins-root-docs}/seed_run.png[caption="Step 2: ", title="Click the 'Build with parameters'"]

{nbsp}
{nbsp}

image::{jenkins-root-docs}/seed.png[caption="Step 3: ", title="The `REPOS` parameter should already contain your forked repos (you'll have more properties than the ones in the screenshot)"]

{nbsp}
{nbsp}

image::{jenkins-root-docs}/seed_built.png[caption="Step 4: ", title="This is how the results of seed should look like"]

[[jenkins-pipeline-k8s]]
==== Run the `github-webhook` pipeline

We already created the seed job for you, but you have to run it. When you do
run it, you have to provide some properties. By default, we create a seed that
has all the properties options, but you can delete most of it. If you
set the properties as global environment variables, you have to remove them from the
seed.

To run the demo, provide a comma-separated
 list of URLs of the two aforementioned forks (`github-webhook` and `github-analytics`) in the `REPOS` variable.

The following images shows the steps involved:

{nbsp}
{nbsp}

image::{jenkins-root-docs}/seed_views.png[caption="Step 1: ", title="Click the 'github-webhook' view"]

{nbsp}
{nbsp}

image::{jenkins-root-docs}/pipeline_run.png[caption="Step 2: ", title="Run the pipeline"]

{nbsp}
{nbsp}

IMPORTANT: If your build fails on *deploy previous version to stage* due to a missing jar,
that means that you forgot to clear the tags in your repository. Typically, that happens because
you removed the Artifactory volume with a deployed jar while a tag in the repository still points there.
See <<tags,here>> for how to remove the tag.

{nbsp}
{nbsp}

image::{jenkins-root-docs}/pipeline_manual.png[caption="Step 3: ", title="Click the manual step to go to stage (remember about killing the apps on test env). To do this click the *ARROW* next to the job name"]

{nbsp}
{nbsp}

IMPORTANT: Servers often run run out of resources at the stage step.
For that reason, we suggest killing all applications on test. See the <<faq,FAQ>> for more detail.

{nbsp}
{nbsp}

image::{jenkins-root-docs}/pipeline_finished.png[caption="Step 4: ", title="The full pipeline should look like this"]

{nbsp}
{nbsp}

[[declarative-pipeline-k8s]]
=== Declarative pipeline & Blue Ocean

You can also use the https://jenkins.io/doc/book/pipeline/syntax/[declarative pipeline] approach with the
https://jenkins.io/projects/blueocean/[Blue Ocean UI].

The Blue Ocean UI is available under the `blue/` URL (for example, for Docker Machine-based setup: `http://192.168.99.100:8080/blue`).

The following images show the various steps involved:

{nbsp}
{nbsp}

image::{jenkins-root-docs}/blue_1.png[caption="Step 1: ", title="Open Blue Ocean UI and click on `github-webhook-declarative-pipeline`"]

{nbsp}
{nbsp}

image::{jenkins-root-docs}/blue_2.png[caption="Step 2: ", title="Your first run will look like this. Click `Run` button"]

{nbsp}
{nbsp}

image::{jenkins-root-docs}/blue_3.png[caption="Step 3: ", title="Enter parameters required for the build and click `run`"]

{nbsp}
{nbsp}

image::{jenkins-root-docs}/blue_4.png[caption="Step 4: ", title="A list of pipelines will be shown. Click your first run."]

{nbsp}
{nbsp}

image::{jenkins-root-docs}/blue_5.png[caption="Step 5: ", title="State if you want to go to production or not and click `Proceed`"]

{nbsp}
{nbsp}

image::{jenkins-root-docs}/blue_6.png[caption="Step 6: ", title="The build is in progress..."]

{nbsp}
{nbsp}

image::{jenkins-root-docs}/blue_7.png[caption="Step 7: ", title="The pipeline is done!"]

{nbsp}
{nbsp}


IMPORTANT: There is no possibility of restarting a pipeline from a specific stage after failure.
See https://issues.jenkins-ci.org/browse/JENKINS-33846[this issue] for more information

WARNING: Currently, there is no way to introduce manual steps in a performant way. Jenkins
blocks an executor when a manual step is required. That means that you run out of executors
pretty quickly. See https://issues.jenkins-ci.org/browse/JENKINS-36235[this issue]
and http://stackoverflow.com/questions/42561241/how-to-wait-for-user-input-in-a-declarative-pipeline-without-blocking-a-heavywei[this StackOverflow question]
for more information.

[[optional-steps-k8s]]
=== Jenkins Kubernetes customization

You can customize Jenkins for Cloud Foundry by setting a variety of environment variables.

NOTE: You need not see all the environment variables described in this section to run the demo. They are needed only
when you want to make custom changes.

[[all-env-vars-k8s]]
==== All env vars

The environment variables that are used in all of the jobs are as follows:

[frame="topbot",options="header,footer"]
|======================
|Property Name  | Property Description | Default value
|`DOCKER_REGISTRY_ORGANIZATION` | Name of the docker organization to which Docker images should be deployed | `scpipelines`
|`DOCKER_REGISTRY_CREDENTIAL_ID` | Credential ID used to push Docker images | `docker-registry`
|`DOCKER_SERVER_ID` | Server ID in `settings.xml` and Maven builds | `docker-repo`
|`DOCKER_EMAIL` | Email used to connect to Docker registry and Maven builds | `change@me.com`
|`DOCKER_REGISTRY_ORGANIZATION` | URL of the Kubernetes cluster for the test environment | `scpipelines`
|`DOCKER_REGISTRY_URL` | URL of the docker registry | `https://index.docker.io/v1/`
|`PAAS_TEST_API_URL` | URL of the API of the Kubernetes cluster for the test environment | `192.168.99.100:8443`
|`PAAS_STAGE_API_URL` | URL of the API of the Kubernetes cluster for the stage environment  | `192.168.99.100:8443`
|`PAAS_PROD_API_URL` | URL of the API of the Kubernetes cluster for the prod environment | `192.168.99.100:8443`
|`PAAS_TEST_CA_PATH` | Path to the certificate authority for test the environment | `/usr/share/jenkins/cert/ca.crt`
|`PAAS_STAGE_CA_PATH` | Path to the certificate authority for stage the environment | `/usr/share/jenkins/cert/ca.crt`
|`PAAS_PROD_CA_PATH` | Path to the certificate authority for the prod environment | `/usr/share/jenkins/cert/ca.crt`
|`PAAS_TEST_CLIENT_CERT_PATH` | Path to the client certificate for the test environment | `/usr/share/jenkins/cert/apiserver.crt`
|`PAAS_STAGE_CLIENT_CERT_PATH` | Path to the client certificate for the stage environment | `/usr/share/jenkins/cert/apiserver.crt`
|`PAAS_PROD_CLIENT_CERT_PATH` | Path to the client certificate for the prod environment | `/usr/share/jenkins/cert/apiserver.crt`
|`PAAS_TEST_CLIENT_KEY_PATH` | Path to the client key for the test environment | `/usr/share/jenkins/cert/apiserver.key`
|`PAAS_STAGE_CLIENT_KEY_PATH` | Path to the client key for the stage environment | `/usr/share/jenkins/cert/apiserver.key`
|`PAAS_PROD_CLIENT_KEY_PATH` | Path to the client key for the test environment | `/usr/share/jenkins/cert/apiserver.key`
|`PAAS_TEST_CLIENT_TOKEN_PATH` | Path to the file containing the token for the test environment |
|`PAAS_STAGE_CLIENT_TOKEN_PATH` | Path to the file containing the token for the stage environment |
|`PAAS_PROD_CLIENT_TOKEN_PATH` | Path to the file containing the token for the prod environment |
|`PAAS_TEST_CLIENT_TOKEN_ID` | ID of the credential containing access token for test environment |
|`PAAS_STAGE_CLIENT_TOKEN_ID` | ID of the credential containing access token for the stage environment |
|`PAAS_PROD_CLIENT_TOKEN_ID` | ID of the credential containing access token for the prod environment |
|`PAAS_TEST_CLUSTER_NAME` | Name of the cluster for the test environment | `minikube`
|`PAAS_STAGE_CLUSTER_NAME` | Name of the cluster for the stage environment | `minikube`
|`PAAS_PROD_CLUSTER_NAME` | Name of the cluster for the prod environment | `minikube`
|`PAAS_TEST_CLUSTER_USERNAME` | Name of the user for the test environment | `minikube`
|`PAAS_STAGE_CLUSTER_USERNAME` | Name of the user for the stage environment | `minikube`
|`PAAS_PROD_CLUSTER_USERNAME` | Name of the user for the prod environment | `minikube`
|`PAAS_TEST_SYSTEM_NAME` | Name of the system for the test environment | `minikube`
|`PAAS_STAGE_SYSTEM_NAME` | Name of the system for the stage environment | `minikube`
|`PAAS_PROD_SYSTEM_NAME` | Name of the system for the prod environment | `minikube`
|`PAAS_TEST_NAMESPACE` | Namespace for the test environment | `sc-pipelines-test`
|`PAAS_STAGE_NAMESPACE` | Namespace for the stage environment | `sc-pipelines-stage`
|`PAAS_PROD_NAMESPACE` | Namespace for the prod environment | `sc-pipelines-prod`
|`KUBERNETES_MINIKUBE` | Whether to connect to Minikube | `true`
|`REPO_WITH_BINARIES_FOR_UPLOAD` | URL of the repository with the deployed jars | `http://artifactory:8081/artifactory/libs-release-local`
|`REPO_WITH_BINARIES_CREDENTIAL_ID` | Credential ID used for the repository with jars | `repo-with-binaries`
|`M2_SETTINGS_REPO_ID` | The ID of server from Maven `settings.xml` | `artifactory-local`
|`JDK_VERSION` | The name of the JDK installation | `jdk8`
|`PIPELINE_VERSION` | The version of the pipeline (ultimately, also the version of the jar) | `1.0.0.M1-${GROOVY,script ="new Date().format('yyMMdd_HHmmss')"}-VERSION`
|`GIT_EMAIL` | The email used by Git to tag the repository | `email@example.com`
|`GIT_NAME` | The name used by Git to tag the repository | `Pivo Tal`
|`AUTO_DEPLOY_TO_STAGE` | Whether deployment to stage be automatic | `false`
|`AUTO_DEPLOY_TO_PROD` | Whether deployment to prod be automatic | `false`
|`API_COMPATIBILITY_STEP_REQUIRED` | Whether the API compatibility step is required | `true`
|`DB_ROLLBACK_STEP_REQUIRED` | Whether the DB rollback step is present | `true`
|`DEPLOY_TO_STAGE_STEP_REQUIRED` | Whether the deploy-to-stage step is present | `true`
|`BUILD_OPTIONS` | Additional options you would like to pass to the Maven / Gradle build |
|======================

=== Preparing to Connect to GCE

IMPORTANT: Skip this step if you do not use GCE

In order to use GCE, we need to have `gcloud` running. If you already have the
CLI installed, skip this step. If not run the following command to have the CLI
downloaded and an installer started:

====
[source,bash]
----
$ ./tools/k8s-helper.sh download-gcloud
----
====

Next, configure `gcloud`. Run `gcloud init` and log in
to your cluster. You are redirected to a login page. Pick the
proper Google account and log in.

Pick an existing project or create a new one.

Go to your platform page (click on `Container Engine`) in GCP and connect to your cluster with the following values:

====
[source,bash]
----
$ CLUSTER_NAME=...
$ ZONE=us-east1-b
$ PROJECT_NAME=...
$ gcloud container clusters get-credentials ${CLUSTER_NAME} --zone ${ZONE} --project ${PROJECT_NAME}
$ kubectl proxy
----
====

The Kubernetes dashboard runs at `http://localhost:8001/ui/`.

We need a Persistent Disk for our Jenkins installation. Create it as follows:

====
[source,bash]
----
$ ZONE=us-east1-b
$ gcloud compute disks create --size=200GB --zone=${ZONE} sc-pipelines-jenkins-disk
----
====

Once the disk has been created, you need to format it. See
the instructions at https://cloud.google.com/compute/docs/disks/add-persistent-disk#formatting

=== Connecting to a Kubo or GCE Cluster

IMPORTANT: Skip this step if you do not use Kubo or GCE

This section describes how to deploy Jenkins and
Artifactory to a Kubernetes cluster deployed with Kubo.

TIP: To see the dashboard, run `kubectl proxy` and access `localhost:8081/ui`.

. Log in to the cluster.
. Deploy Jenkins and Artifactory to the cluster:
* `./tools/k8s-helper.sh setup-tools-infra-vsphere` for a cluster deployed on VSphere
* `./tools/k8s-helper.sh setup-tools-infra-gce` for a cluster deployed to GCE
. Forward the ports so that you can access the Jenkins UI from your local machine, by using the following settings
====
[source,bash]
$ NAMESPACE=default
$ JENKINS_POD=jenkins-1430785859-nfhx4
$ LOCAL_PORT=32044
$ CONTAINER_PORT=8080
$ kubectl port-forward --namespace=${NAMESPACE} ${JENKINS_POD} ${LOCAL_PORT}:${CONTAINER_PORT}
----
====
. Go to `Credentials`, click `System` and `Global credentials`, as the following image shows:
image::{jenkins-root-docs}/kubo_credentials.png[caption="Click `Global credentials`"]
. Update `git`, `repo-with-binaries` and `docker-registry` credentials
. Run the `jenkins-pipeline-k8s-seed` seed job and fill it out with the following data
. Put `kubernetes.default:443` here (or `KUBERNETES_API:KUBERNETES_PORT`)
** `PAAS_TEST_API_URL`
** `PAAS_STAGE_API_URL`
** `PAAS_PROD_API_URL`
. Put `/var/run/secrets/kubernetes.io/serviceaccount/ca.crt` data here:
** `PAAS_TEST_CA_PATH`
** `PAAS_STAGE_CA_PATH`
** `PAAS_PROD_CA_PATH`
. Uncheck the `Kubernetes Minikube` value.
- Clear the following variables:
** `PAAS_TEST_CLIENT_CERT_PATH`
** `PAAS_STAGE_CLIENT_CERT_PATH`
** `PAAS_PROD_CLIENT_CERT_PATH`
** `PAAS_TEST_CLIENT_KEY_PATH`
** `PAAS_STAGE_CLIENT_KEY_PATH`
** `PAAS_PROD_CLIENT_KEY_PATH`
. Set `/var/run/secrets/kubernetes.io/serviceaccount/token` value to these variables:
** `PAAS_TEST_CLIENT_TOKEN_PATH`
** `PAAS_STAGE_CLIENT_TOKEN_PATH`
** `PAAS_STAGE_CLIENT_TOKEN_PATH`
* Set the cluster name to these variables (you can get the cluster name by calling `kubectl config current-context`):
** `PAAS_TEST_CLUSTER_NAME`
** `PAAS_STAGE_CLUSTER_NAME`
** `PAAS_PROD_CLUSTER_NAME`
. Set the system name to these variables (you can get the system name by calling `kubectl config current-context`):
** `PAAS_TEST_SYSTEM_NAME`
** `PAAS_STAGE_SYSTEM_NAME`
** `PAAS_PROD_SYSTEM_NAME`
. Update the `DOCKER_EMAIL` property with your email address.
. Update the `DOCKER_REGISTRY_ORGANIZATION` with your Docker organization name.
. If you do not want to upload the images to DockerHub, update `DOCKER_REGISTRY_URL`.
image::{jenkins-root-docs}/pks_seed.png[caption="Example of a filled out seed job"]
. Run the pipeline

== Kubernetes Setup

This section describes how to set up Kubernetes.

=== Kubernetes CLI Installation

First, you need to install the `kubectl` command-line interface (CLI).

[[kubernetes-cli-script]]
==== Script Installation

You can use the `tools/k8s-helper.sh` script to install `kubectl`. To do so, run the following script:

====
[source,bash]
----
$ ./tools/minikube-helper download-kubectl
----
====

Then the `kubectl` gets downloaded.

[[kubernetes-cli-manual]]
==== Manual Installation

You can perform a manual installation for either OSX or Linux.

===== Example for OSX

The following listing shows how to manually install on OSX:

====
[source,bash]
$ curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl
$ chmod +x ./kubectl
$ sudo mv ./kubectl /usr/local/bin/kubectl
----
====

===== Example for Linux

The following listing shows how to manually install on Linux:

====
[source,bash]
----
$ curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
$ chmod +x ./kubectl
$ sudo mv ./kubectl /usr/local/bin/kubectl
----
====

See https://kubernetes.io/docs/tasks/tools/install-kubectl/[this page] for more information.

[[start-minikube-k8s]]
=== Kubernetes Cluster Setup

We need a cluster of Kubernetes. The best choice is https://github.com/kubernetes/minikube[Minikube].

TIP: You can skip this step if you have a Kubernetes cluster installed and do not
want to use Minikube. In that case, the only thing you have to do is to set up spaces.

WARNING: Servers often run run out of resources at the stage step.
If that happens, <<jenkins-resources-k8s,clear some apps from PCF Dev and continue>>.

[[kubernetes-minikube-script]]
==== Script Installation

You can use the `tools/k8s-helper.sh` script to install `Minikube`. To do so, run the following script:

====
[source,bash]
----
$ ./tools/minikube-helper download-minikube
----
====

Then the `Minikube` cluster gets downloaded.

[[kubernetes-minikube-manual]]
==== Manual Installation

You can perform a manual installation for either OSX or Linux.

===== Example for OSX

The following listing shows how to manually install on OSX:

====
[source,bash]
----
$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.20.0/minikube-darwin-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/
----
====

Feel free to skip running `sudo mv minikube /usr/local/bin` if you want to add minikube to your path manually.

==== Example for Linux

The following listing shows how to manually install on Linux:

====
[source,bash]
----
$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.20.0/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/
----
====

You can skip running `sudo mv minikube /usr/local/bin` if you want to add minikube to your path manually.
See https://github.com/kubernetes/minikube/releases[this page] for more information on the installation.

=== Run Minikube

To start Kubernetes on your local box, run `minikube start`.

To add the dashboard, run `minikube dashboard`.

=== Certificates and Workers

==== Minikube Certificates and Workers

By default, if you install Minikube, all the certificates get installed in your
`~/.minikube` folder. Your `kubectl` configuration under `~/.kube/config` also
gets updated to use Minikube.

==== Manual Certificates and Workers Setup

NOTE: If you want to run the default demo setup, you can skip this section.

To target a given Kubernetes instance, you need to pass around Certificate Authority
key and also user keys.

You can read more about the instructions on how to generate those keys https://coreos.com/kubernetes/docs/latest/openssl.html[here].
Generally speaking, if you have a Kubernetes installation (such as `minikube`), this step
has already been done for you. Now you can reuse those keys on the workers.

The following inormation has been extracted from the https://coreos.com/kubernetes/docs/latest/configure-kubectl.html[Kubernetes official documentation].

Configure `kubectl` to connect to the target cluster using the following commands, replacing the following values:

* Replace `${MASTER_HOST}` with the master node address or name used in previous steps.
* Replace `${CA_CERT}` with the absolute path to the `ca.pem` created in previous steps.
* Replace `${ADMIN_KEY}` with the absolute path to the `admin-key.pem` created in previous steps.
* Replace `${ADMIN_CERT}` with the absolute path to the `admin.pem` created in previous steps.

The following commands show how to perform these steps:

====
[source,bash]
----
$ kubectl config set-cluster default-cluster --server=https://${MASTER_HOST} --certificate-authority=${CA_CERT}
$ kubectl config set-credentials default-admin --certificate-authority=${CA_CERT} --client-key=${ADMIN_KEY} --client-certificate=${ADMIN_CERT}
$ kubectl config set-context default-system --cluster=default-cluster --user=default-admin
$ kubectl config use-context default-system
----
====

=== Generate Minikube Namespaces

With the Minikube cluster running, we need to generate namespaces. To do so, run the
`./tools/k8s-helper.sh setup-namespaces`.


== The demo setup (Kubernetes)

The demo uses two applications: https://github.com/spring-cloud-samples/github-webhook-kubernetes/[Github Webhook]
and https://github.com/spring-cloud-samples/github-analytics-kubernetes/[Github analytics code]. The following
image shows how these application communicate with each other:

image::{demo-root-docs}/demo.png[caption="The overview of the demo: ", title="Github Webhook listens to HTTP calls and sends a message to Github Analytics"]

{nbsp}
{nbsp}

For the demo scenario we have two applications. `Github Analytics` and `Github Webhook`.
Let's imagine a case where Github is emitting events via HTTP. `Github Webhook` has
an API that could register to such hooks and receive those messages. Once this happens
 `Github Webhook` sends a message by RabbitMQ to a channel. `Github Analytics` is
 listening to those messages and stores them in a MySQL database.

image::{demo-root-docs}/demo_metrics.png[caption="Gathering metrics: ", title="Github Analytics exposes metrics that are polled by Prometheus"]

{nbsp}
{nbsp}

`Github Analytics` has its KPIs (Key Performance Indicators) monitored. In the case
of that application the KPI is number of issues.

image::{demo-root-docs}/demo_alerting.png[caption="Alerting over metrics: ", title="Grafana alerts Slack over Prometheus metrics"]

{nbsp}
{nbsp}

Let's assume that if we go below the threshold of X issues then an alert should be
sent to Slack.

=== Deploying Production Applications to Minikube

In a real-world scenario, we would not want to automatically provision services such as
RabbitMQ, MySQL, or Eureka each time we deploy a new application to production. Typically,
production is provisioned manually (often by using automated solutions). In our case, before
you deploy to production, you can provision the `sc-pipelines-prod` namespace by using the
 `k8s-helper.sh`. To do so, call the following script:

====
[source,bash]
----
$ ./k8s-helper.sh setup-prod-infra
----
====

=== Running Prometheus on Kubernetes

Use Helm to install Prometheus. Later in this demo, we point it to the services
deployed to our cluster.

Create a file called `values.yaml` with the following content:

.values.yaml
====
[source,yml]
----
rbac:
  create: false

alertmanager:
  ## If false, alertmanager will not be installed
  ##
  enabled: true

  # Defines the serviceAccountName to use when `rbac.create=false`
  serviceAccountName: default

  ## alertmanager container name
  ##
  name: alertmanager

  ## alertmanager container image
  ##
  image:
    repository: prom/alertmanager
    tag: v0.9.1
    pullPolicy: IfNotPresent

  ## Additional alertmanager container arguments
  ##
  extraArgs: {}

  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug
  ## so that the various internal URLs are still able to access as they are in the default case.
  ## (Optional)
  baseURL: ""

  ## Additional alertmanager container environment variable
  ## For instance to add a http_proxy
  ##
  extraEnv: {}

  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.alertmanager.configMapOverrideName}}
  ## Defining configMapOverrideName will cause templates/alertmanager-configmap.yaml
  ## to NOT generate a ConfigMap resource
  ##
  configMapOverrideName: ""

  ingress:
    ## If true, alertmanager Ingress will be created
    ##
    enabled: false

    ## alertmanager Ingress annotations
    ##
    annotations: {}
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    ## alertmanager Ingress hostnames
    ## Must be provided if Ingress is enabled
    ##
    hosts: []
    #   - alertmanager.domain.com

    ## alertmanager Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls: []
    #   - secretName: prometheus-alerts-tls
    #     hosts:
    #       - alertmanager.domain.com

  ## Alertmanager Deployment Strategy type
  # strategy:
  #   type: Recreate

  ## Node labels for alertmanager pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  persistentVolume:
    ## If true, alertmanager will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## alertmanager data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## alertmanager data Persistent Volume Claim annotations
    ##
    annotations: {}

    ## alertmanager data Persistent Volume existing claim name
    ## Requires alertmanager.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: ""

    ## alertmanager data Persistent Volume mount root path
    ##
    mountPath: /data

    ## alertmanager data Persistent Volume size
    ##
    size: 2Gi

    ## alertmanager data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

    ## Subdirectory of alertmanager data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ""

  ## Annotations to be added to alertmanager pods
  ##
  podAnnotations: {}

  replicaCount: 1

  ## alertmanager resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 10m
    #   memory: 32Mi
    # requests:
    #   cpu: 10m
    #   memory: 32Mi

  service:
    annotations: {}
    labels: {}
    clusterIP: ""

    ## List of IP addresses at which the alertmanager service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    # nodePort: 30000
    type: ClusterIP

## Monitors ConfigMap changes and POSTs to a URL
## Ref: https://github.com/jimmidyson/configmap-reload
##
configmapReload:
  ## configmap-reload container name
  ##
  name: configmap-reload

  ## configmap-reload container image
  ##
  image:
    repository: jimmidyson/configmap-reload
    tag: v0.1
    pullPolicy: IfNotPresent

  ## configmap-reload resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}

kubeStateMetrics:
  ## If false, kube-state-metrics will not be installed
  ##
  enabled: true

  # Defines the serviceAccountName to use when `rbac.create=false`
  serviceAccountName: default

  ## kube-state-metrics container name
  ##
  name: kube-state-metrics

  ## kube-state-metrics container image
  ##
  image:
    repository: gcr.io/google_containers/kube-state-metrics
    tag: v1.1.0-rc.0
    pullPolicy: IfNotPresent

  ## Node labels for kube-state-metrics pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Annotations to be added to kube-state-metrics pods
  ##
  podAnnotations: {}

  replicaCount: 1

  ## kube-state-metrics resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 10m
    #   memory: 16Mi
    # requests:
    #   cpu: 10m
    #   memory: 16Mi

  service:
    annotations:
      prometheus.io/scrape: "true"
    labels: {}

    clusterIP: None

    ## List of IP addresses at which the kube-state-metrics service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    type: ClusterIP

nodeExporter:
  ## If false, node-exporter will not be installed
  ##
  enabled: true

  # Defines the serviceAccountName to use when `rbac.create=false`
  serviceAccountName: default

  ## node-exporter container name
  ##
  name: node-exporter

  ## node-exporter container image
  ##
  image:
    repository: prom/node-exporter
    tag: v0.15.0
    pullPolicy: IfNotPresent

  ## Additional node-exporter container arguments
  ##
  extraArgs: {}

  ## Additional node-exporter hostPath mounts
  ##
  extraHostPathMounts: []
    # - name: textfile-dir
    #   mountPath: /srv/txt_collector
    #   hostPath: /var/lib/node-exporter
    #   readOnly: true

  ## Node tolerations for node-exporter scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Node labels for node-exporter pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Annotations to be added to node-exporter pods
  ##
  podAnnotations: {}

  ## node-exporter resource limits & requests
  ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 200m
    #   memory: 50Mi
    # requests:
    #   cpu: 100m
    #   memory: 30Mi

  service:
    annotations:
      prometheus.io/scrape: "true"
    labels: {}

    clusterIP: None

    ## List of IP addresses at which the node-exporter service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    hostPort: 9100
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 9100
    type: ClusterIP

server:
  ## Prometheus server container name
  ##
  name: server

  # Defines the serviceAccountName to use when `rbac.create=false`
  serviceAccountName: default

  ## Prometheus server container image
  ##
  image:
    repository: prom/prometheus
    tag: v1.8.0
    pullPolicy: IfNotPresent

  ## (optional) alertmanager URL
  ## only used if alertmanager.enabled = false
  alertmanagerURL: ""

  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug
  ## so that the various internal URLs are still able to access as they are in the default case.
  ## (Optional)
  baseURL: ""

  ## Additional Prometheus server container arguments
  ##
  extraArgs: {}

  ## Additional Prometheus server hostPath mounts
  ##
  extraHostPathMounts: []
    # - name: certs-dir
    #   mountPath: /etc/kubernetes/certs
    #   hostPath: /etc/kubernetes/certs
    #   readOnly: true

  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.server.configMapOverrideName}}
  ## Defining configMapOverrideName will cause templates/server-configmap.yaml
  ## to NOT generate a ConfigMap resource
  ##
  configMapOverrideName: ""

  ingress:
    ## If true, Prometheus server Ingress will be created
    ##
    enabled: false

    ## Prometheus server Ingress annotations
    ##
    annotations: {}
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    ## Prometheus server Ingress hostnames
    ## Must be provided if Ingress is enabled
    ##
    hosts: []
    #   - prometheus.domain.com

    ## Prometheus server Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls: []
    #   - secretName: prometheus-server-tls
    #     hosts:
    #       - prometheus.domain.com

  ## Server Deployment Strategy type
  # strategy:
  #   type: Recreate

  ## Node tolerations for server scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Node labels for Prometheus server pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  nodeSelector: {}

  persistentVolume:
    ## If true, Prometheus server will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## Prometheus server data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## Prometheus server data Persistent Volume annotations
    ##
    annotations: {}

    ## Prometheus server data Persistent Volume existing claim name
    ## Requires server.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: ""

    ## Prometheus server data Persistent Volume mount root path
    ##
    mountPath: /data

    ## Prometheus server data Persistent Volume size
    ##
    size: 8Gi

    ## Prometheus server data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

    ## Subdirectory of Prometheus server data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ""

  ## Annotations to be added to Prometheus server pods
  ##
  podAnnotations: {}
    # iam.amazonaws.com/role: prometheus

  replicaCount: 1

  ## Prometheus server resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 500m
    #   memory: 512Mi
    # requests:
    #   cpu: 500m
    #   memory: 512Mi

  service:
    annotations: {}
    labels: {}
    clusterIP: ""

    ## List of IP addresses at which the Prometheus server service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    type: ClusterIP

  ## Prometheus server pod termination grace period
  ##
  terminationGracePeriodSeconds: 300

  ## Prometheus data retention period (i.e 360h)
  ##
  retention: ""

pushgateway:
  ## If false, pushgateway will not be installed
  ##
  enabled: true

  ## pushgateway container name
  ##
  name: pushgateway

  ## pushgateway container image
  ##
  image:
    repository: prom/pushgateway
    tag: v0.4.0
    pullPolicy: IfNotPresent

  ## Additional pushgateway container arguments
  ##
  extraArgs: {}

  ingress:
    ## If true, pushgateway Ingress will be created
    ##
    enabled: false

    ## pushgateway Ingress annotations
    ##
    annotations:
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    ## pushgateway Ingress hostnames
    ## Must be provided if Ingress is enabled
    ##
    hosts: []
    #   - pushgateway.domain.com

    ## pushgateway Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls: []
    #   - secretName: prometheus-alerts-tls
    #     hosts:
    #       - pushgateway.domain.com

  ## Node labels for pushgateway pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Annotations to be added to pushgateway pods
  ##
  podAnnotations: {}

  replicaCount: 1

  ## pushgateway resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 10m
    #   memory: 32Mi
    # requests:
    #   cpu: 10m
    #   memory: 32Mi

  service:
    annotations:
      prometheus.io/probe: pushgateway
    labels: {}
    clusterIP: ""

    ## List of IP addresses at which the pushgateway service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 9091
    type: ClusterIP

## alertmanager ConfigMap entries
##
alertmanagerFiles:
  alertmanager.yml: |-
    global:
      # slack_api_url: ''
    receivers:
      - name: default-receiver
        # slack_configs:
        #  - channel: '@you'
        #    send_resolved: true
    route:
      group_wait: 10s
      group_interval: 5m
      receiver: default-receiver
      repeat_interval: 3h
## Prometheus server ConfigMap entries
##
serverFiles:
  alerts: ""
  rules: ""

  prometheus.yml: |-
    rule_files:
      - /etc/config/rules
      - /etc/config/alerts
    scrape_configs:
      - job_name: 'demo-app'
        scrape_interval: 5s
        metrics_path: '/prometheus'
        static_configs:
          - targets:
            - github-analytics.sc-pipelines-prod.svc.cluster.local:8080
      - job_name: prometheus
        static_configs:
          - targets:
            - localhost:9090
      # A scrape configuration for running Prometheus on a Kubernetes cluster.
      # This uses separate scrape configs for cluster components (i.e. API server, node)
      # and services to allow each to use different authentication configs.
      #
      # Kubernetes labels will be added as Prometheus labels on metrics via the
      # `labelmap` relabeling action.
      # Scrape config for API servers.
      #
      # Kubernetes exposes API servers as endpoints to the default/kubernetes
      # service so this uses `endpoints` role and uses relabelling to only keep
      # the endpoints associated with the default/kubernetes service using the
      # default named port `https`. This works for single API server deployments as
      # well as HA API server deployments.
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
          - role: endpoints
        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https
        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        # Keep only the default/kubernetes service endpoints for the https port. This
        # will add targets for each API server which Kubernetes adds an endpoint to
        # the default/kubernetes service.
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https
      - job_name: 'kubernetes-nodes'
        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https
        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics
      # Scrape config for service endpoints.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: If the metrics are exposed on a different port to the
      # service then set this appropriately.
      - job_name: 'kubernetes-service-endpoints'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: (.+)(?::\d+);(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: kubernetes_name
      - job_name: 'prometheus-pushgateway'
        honor_labels: true
        kubernetes_sd_configs:
          - role: service
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
            action: keep
            regex: pushgateway
      # Example scrape config for probing services via the Blackbox Exporter.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/probe`: Only probe services that have a value of `true`
      - job_name: 'kubernetes-services'
        metrics_path: /probe
        params:
          module: [http_2xx]
        kubernetes_sd_configs:
          - role: service
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
            action: keep
            regex: true
          - source_labels: [__address__]
            target_label: __param_target
          - target_label: __address__
            replacement: blackbox
          - source_labels: [__param_target]
            target_label: instance
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: kubernetes_name
      # Example scrape config for pods
      #
      # The relabeling allows the actual pod scrape endpoint to be configured via the
      # following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: (.+):(?:\d+);(\d+)
            replacement: ${1}:${2}
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name
networkPolicy:
  ## Enable creation of NetworkPolicy resources.
  ##
  enabled: false
----
====

Next, create the prometheus installation with the predefined values. To do so, run the following command:

====
[source,bash]
----
$ helm install --name sc-pipelines-prometheus stable/prometheus -f values.yaml
----
====

Then you should see the following output:

====
[source,bash]
----
NOTES:
The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:
sc-pipelines-prometheus-prometheus-server.default.svc.cluster.local


Get the Prometheus server URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace default -l "app=prometheus,component=server" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace default port-forward $POD_NAME 9090


The Prometheus alertmanager can be accessed via port 80 on the following DNS name from within your cluster:
sc-pipelines-prometheus-prometheus-alertmanager.default.svc.cluster.local


Get the Alertmanager URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace default -l "app=prometheus,component=alertmanager" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace default port-forward $POD_NAME 9093


The Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster:
sc-pipelines-prometheus-prometheus-pushgateway.default.svc.cluster.local


Get the PushGateway URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace default -l "app=prometheus,component=pushgateway" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace default port-forward $POD_NAME 9093

For more information on running Prometheus, visit:
https://prometheus.io/
----
====

=== Running Grafana on Kubernetes

Use Helm to install Grafana, by running the following command:

====
[source,bash]
----
$ helm install --name sc-pipelines-grafana stable/grafana
----
====

You should see the following output:

====
[source,bash]
----
NOTES:
1. Get your 'admin' user password by running:

   kubectl get secret --namespace default sc-pipelines-grafana-grafana -o jsonpath="{.data.grafana-admin-password}" | base64 --decode ; echo

2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:

   sc-pipelines-grafana-grafana.default.svc.cluster.local

   Get the Grafana URL to visit by running these commands in the same shell:

     export POD_NAME=$(kubectl get pods --namespace default -l "app=sc-pipelines-grafana-grafana,component=grafana" -o jsonpath="{.items[0].metadata.name}")
     kubectl --namespace default port-forward $POD_NAME 3000

3. Login with the password from step 1 and the username: admin
----
====

Perform the steps listed in the preceding output and add the Grafana's datasource
as Prometheus with the following URL: `http://sc-pipelines-prometheus-prometheus-server.default.svc.cluster.local`

You can pick up the dashboard with the Grafana ID (2471). This is the
default dashboard for the Spring Cloud Pipelines demo apps.

If you have both apps (`github-webhook` and `github-analytics`) running on production,
you can now trigger the messages. Download the JSON with a sample request
from https://github.com/marcingrzejszczak/github-webhook-kubernetes/blob/master/src/test/resources/github-webhook-input/hook-created.json[the github-webhook repository].
Next, pick one of the `github-webhook` pods and forward its port
locally to a port `9876`, as follows:

====
[source,bash]
----
$ kubectl port-forward --namespace=sc-pipelines-prod $( kubectl get pods --namespace=sc-pipelines-prod | grep github-webhook | head -1 | awk '{print $1}' ) 9876:8080
----
====

Next, send a couple of requests (more than four), by using cURL as follows:

====
[source,bash]
----
$ curl -X POST http://localhost:9876/ -d @path/to/issue-created.json \
--header "Content-Type: application/json"
----
====

Then, if you use Grafana, you can see that you went above the threshold.

== Building the Project

This section covers how to build the project. It covers:

* <<building-project-setup>>
* <<building-prerequisites>>
* <<building-bats-submodules>>
* <<building-build-and-test>>
* <<building-generate-documentation>>
* <<building-distributions>>
* <<building-making-a-release>>

[[building-project-setup]]
=== Project Setup

The following diagram shows the folder structure of Spring Cloud Pipelines:

====
[source,bash]
----
.
├── common
├── concourse
├── dist
├── docs
├── docs-sources
└── jenkins
----
====

In the `common` folder, you can find all the Bash scripts that contain the pipeline logic. These
scripts are reused by both the Concourse and Jenkins pipelines.

In the `concourse` folder, you can find all the necessary scripts and setup information to run the Concourse demo.

In the `dist` folder, you can find the packaged sources of the project. Since the package
contains no tests or documentation, it is extremely small and can be used in the pipelines.

In the `docs` folder, you can find the whole generated documentation of the project.

In the `docs-source` folder, you can find the sources required to generate the documentation.

In the `jenkins` folder, you can find all the necessary scripts and setup information to run the Jenkins demo.

[[building-prerequisites]]
=== Prerequisites

As prerequisites, you need to have http://www.shellcheck.net/[shellcheck],
https://github.com/sstephenson/bats[bats], https://stedolan.github.io/jq/[jq]
and https://rubyinstaller.org/downloads/[ruby] installed. If you use a Linux
machine, `bats` and `shellcheck` are installed for you.

To install the required software on Linux, type the following command:

====
[source,bash]
----
$ sudo apt-get install -y ruby jq
----
====

If you use a Mac, run the following commands to install the missing software:

[source,bash]
----
$ brew install jq
$ brew install ruby
$ brew install bats
$ brew install shellcheck
----

[[building-bats-submodules]]
=== Bats Submodules

To make `bats` work properly, we needed to attach Git submodules. To have them
initialized, either clone the project or (if you have already cloned the project)
pull to update it. The following command clones the project:

====
[source,bash]
----
$ git clone --recursive https://github.com/spring-cloud/spring-cloud-pipelines.git
----
====

The following commands pull the project:

====
[source,bash]
----
$ git submodule init
$ git submodule update
----
====

If you forget about this step, Gradle runs these steps for you.

[[building-build-and-test]]
=== Build and test

Once you have installed all the prerequisites, you can run the following command to build and test the project:

====
[source,bash]
----
$ ./gradlew clean build
----
====

[[building-generate-documentation]]
=== Generate Documentation

To generate the documentation, run the following command:

====
[source,bash]
----
$ ./gradlew generateDocs
----
====

[[building-distributions]]
=== Distributions

Spring Cloud Pipelines has a lot of tests, including Git repositories. Those
and the documentation "`weigh`" a lot. That is why, under the `dist` folder, we
publish `zip` and `tar.gz` distributions of the sources without tests and documentation.
Whenever we release a distribution, we attach a `VERSION` file to it that contains
build and SCM information (build time, revision number, and other details). To skip the
distribution generation pass the `skipDist` property on the command line, as follows:

====
[source,bash]
----
$ ./gradlew build -PskipDist
----
====

[[building-making-a-release]]
=== Making a Release

You can run the `release` task to automatically test the project,
build the distributions, change the versions, build the docs, upload the docs to Spring Cloud Static,
tag the repo, and then revert the changed versions back to default. To do so, run the
following command:

====
[source,bash]
----
$ ./gradlew release -PnewVersion=1.0.0.RELEASE
----
====

== Releasing the Project

This section covers how to release the project by publishing a Docker image.

=== Publishing A Docker Image

When doing a release, you also need to push a Docker image to Dockerhub.
From the project root, run the following commands, replacing `<version>` with the
version of the release:

====
[source,bash]
----
$ docker login
$ docker build -t springcloud/spring-cloud-pipeline-jenkins:<version> ./jenkins
$ docker push springcloud/spring-cloud-pipeline-jenkins:<version>
----
====

== CI Server Worker Prerequisites

Spring Cloud Pipelines uses Bash scripts extensively. The following list shows the software
that needs to be installed on a CI server worker for the build to pass:

====
[source,bash]
----
 apt-get -y install \
    bash \
    git \
    tar \
    zip \
    curl \
    ruby \
    wget \
    unzip \
    python \
    jq
----
====

TIP: In the demo setup all of these libraries are already installed.

IMPORTANT: In the Jenkins case, you also need `bats` and `shellcheck`. They are not
included in the preceding list, because the versions installed by Linux distributions might be old.
That is why this project's Gradle tasks download the latest versions of both libraries
for you.


[[jenkins_faq]]
== Jenkins FAQ

This section provides answers to the most frequently asked questions about using Jenkins with Spring Cloud Pipelines.

Pipeline version contains ${PIPELINE_VERSION}::
You can check the Jenkins logs and see the following warning:
+
====
[source,bash]
----
WARNING: Skipped parameter `PIPELINE_VERSION` as it is undefined on `jenkins-pipeline-sample-build`.
	Set `-Dhudson.model.ParametersAction.keepUndefinedParameters`=true to allow undefined parameters
	to be injected as environment variables or
	`-Dhudson.model.ParametersAction.safeParameters=[comma-separated list]`
	to whitelist specific parameter names, even though it represents a security breach
----
====
+
To fix it, you have to do exactly what the warning suggests. Also, you should ensure that the `Groovy token macro processing`
checkbox is set.

Pipeline version is not passed to the build::
You can see that the Jenkins version is properly set. However, in the build version, it is still snapshot and
the `echo "${PIPELINE_VERSION}"` does not print anything.
+
You can check the Jenkins logs and see the following warning:
+
====
[source,bash]
----
WARNING: Skipped parameter `PIPELINE_VERSION` as it is undefined on `jenkins-pipeline-sample-build`.
	Set `-Dhudson.model.ParametersAction.keepUndefinedParameters`=true to allow undefined parameters
	to be injected as environment variables or
	`-Dhudson.model.ParametersAction.safeParameters=[comma-separated list]`
	to whitelist specific parameter names, even though it represents a security breach
----
====
+
To fix it, you have to do exactly what the warning suggests.

The build times out with `pipeline.sh` information::
This is a Docker compose issue. The problem is that for some reason, only in Docker, the execution of
Java hangs. However, it hangs randomly and only the first time you try to run the pipeline.
+
The solution to this issue is to run the pipeline again. If it passes once,
it will pass for any subsequent build.
+
Another thing that you can try is to run it with plain Docker. That helps sometimes.

Can I use the pipeline for some other repositories?::
Yes. You can pass the `REPOS` variable with a comma-separated list of
`project_name$project_url` format. If you do not provide the `PROJECT_NAME`, the
repository name is extracted and used as the name of the project.
+
For example, a `REPOS` value equal to `https://github.com/spring-cloud-samples/github-analytics,https://github.com/spring-cloud-samples/github-webhook`
results in the creation of pipelines with root names `github-analytics` and `github-webhook`.
+
For example, a `REPOS` equal to `myanalytics$https://github.com/spring-cloud-samples/github-analytics,myfeed$https://github.com/spring-cloud-samples/atom-feed`
results in the creation of pipelines with root names `myanalytics` for `github-analytics`
and `myfeed` for `github-webhook`.

Can this work for ANY project out of the box?::
Not really. This is an "`opinionated pipeline`". That is why we took some
opinionated decisions, such as:
+
* Using Spring Cloud, Spring Cloud Contract Stub Runner, and Spring Cloud Eureka.
* Application deployment to Cloud Foundry.
* Maven, including:
** Using the Maven Wrapper.
** Artifacts deployment by using `./mvnw clean deploy`.
** The `stubrunner.ids` property to retrieve the list of collaborators for which stubs should be downloaded.
** Running smoke tests on a deployed app with the `smoke` Maven profile.
** Running end to end tests on a deployed app with the `e2e` Maven profile.
* Gradle (in the `github-analytics` application check in the `gradle/pipeline.gradle` file), including:
** Using the Gradlew Wrapper.
** A `deploy` task for artifacts deployment.
** Running smoke tests on a deployed application with the `smoke` task.
** Running end to end tests on a deployed application with the `e2e` task.
** A `groupId` task to retrieve group ID.
** An `artifactId` task to retrieve artifact ID.
** A `currentVersion` task to retrieve the current version.
** A `stubIds` task to retrieve the list of collaborators for which stubs should be downloaded.
+
This is the initial approach that can be easily changed in the future.

Can I modify this to reuse in my project?::
Yes. It is open-source. The important thing is that the core part of the logic is written
in Bash scripts. That way, in the majority of cases, you could change only the bash
scripts without changing the whole pipeline.

The rollback step fails due to a missing JAR::
[[jenkins_tags]] You must have pushed some tags and have removed the Artifactory volume that
contained them. To fix this, remove the tags by using the following command:
+
====
[source,bash]
----
git tag -l | xargs -n 1 git push --delete origin
----
====

I want to provide a different JDK version.::
* By default, we assume that you have configured a JDK with an ID of `jdk8`.
* If you want a different one, override the `JDK_VERSION` environment variable to point to the proper one.
+
TIP: The docker image comes in with Java installed at `/usr/lib/jvm/java-8-openjdk-amd64`.
You can go to `Global Tools` and create a JDK with an ID of `jdk8` and set `JAVA_HOME`
to `/usr/lib/jvm/java-8-openjdk-amd64`.

To change the default settings, follow the steps shown in the following images:

image::{jenkins-root-docs}/manage_jenkins.png[caption="Step 1: ", title="Click 'Manage Jenkins'"]

image::{jenkins-root-docs}/global_tool.png[caption="Step 2: ", title="Click 'Global Tool'"]

image::{jenkins-root-docs}/jdk_installation.png[caption="Step 3: ", title="Click 'JDK Installations'"]

image::{jenkins-root-docs}/jdk.png[caption="Step 4: ", title="Fill out JDK Installation with path to your JDK"]

[[groovy-token-macro]]
How can I enable groovy token macro processing?::
We scripted that. However, if you need to this manually, follow the steps shown in the following images:

image::{jenkins-root-docs}/manage_jenkins.png[caption="Step 1: ", title="Click 'Manage Jenkins'"]

image::{jenkins-root-docs}/configure_system.png[caption="Step 2: ", title="Click 'Configure System'"]

image::{jenkins-root-docs}/groovy_token.png[caption="Step 3: ", title="Click 'Allow token macro processing'"]

How can I make deployment to stage and prod be automatic?::
Set the relevant property or environment variable to `true`:
+
* `AUTO_DEPLOY_TO_STAGE` to automatically deploy to stage.
* `AUTO_DEPLOY_TO_PROD` to automatically deploy to prod.
+
How can I skip testing API compatibility?::
Set the `API_COMPATIBILITY_STEP_REQUIRED` environment variable
to `false` and re-run the seed (you can pick it from the seed
job's properties, too).

I can't tag the repo.::
You may get an error similar to the following:
+
====
[source,bash]
----
19:01:44 stderr: remote: Invalid username or password.
19:01:44 fatal: Authentication failed for 'https://github.com/marcingrzejszczak/github-webhook/'
19:01:44
19:01:44 	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandIn(CliGitAPIImpl.java:1740)
19:01:44 	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandWithCredentials(CliGitAPIImpl.java:1476)
19:01:44 	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.access$300(CliGitAPIImpl.java:63)
19:01:44 	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$8.execute(CliGitAPIImpl.java:1816)
19:01:44 	at hudson.plugins.git.GitPublisher.perform(GitPublisher.java:295)
19:01:44 	at hudson.tasks.BuildStepMonitor$3.perform(BuildStepMonitor.java:45)
19:01:44 	at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:779)
19:01:44 	at hudson.model.AbstractBuild$AbstractBuildExecution.performAllBuildSteps(AbstractBuild.java:720)
19:01:44 	at hudson.model.Build$BuildExecution.post2(Build.java:185)
19:01:44 	at hudson.model.AbstractBuild$AbstractBuildExecution.post(AbstractBuild.java:665)
19:01:44 	at hudson.model.Run.execute(Run.java:1745)
19:01:44 	at hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:43)
19:01:44 	at hudson.model.ResourceController.execute(ResourceController.java:98)
19:01:44 	at hudson.model.Executor.run(Executor.java:404)
----
====
+
Most likely, you passed a wrong password. Check the <<jenkins_credentials,credentials>> section
for how to update your credentials.

I am unauthorized to deploy infrastructure jars.::
Most likely, you forgot to update your local `settings.xml` file with the Artifactory's
setup. Check out <<jenkins_settings,this section of the docs and update your `settings.xml` file>>.

Signing Artifacts::
In some cases, it may be required that, when you perform a release, that the artifacts be signed
before you push them to the repository.
To do this, you  need to import your GPG keys into the Docker image that runs Jenkins.
This can be done by placing a file called `public.key` that contains your public key
and a file called `private.key` that contains your private key in the `seed` directory.
These keys are imported by the `init.groovy` script runs when Jenkins starts.

Using SSH keys for Git::
The seed job checks whether an environment variable called `GIT_USE_SSH_KEY` is set to `true`. If it is `true`, the
environment variable called `GIT_SSH_CREDENTIAL_ID` is chosen as the one that contains the
ID of the credential that contains SSH private key. By default, `GIT_CREDENTIAL_ID` is picked
as the one that contains the username and password to connect to git.
+
You can set these values in the seed job by filling out the form and toggling a checkbox.

Deploy to stage fails and does not redeploy a service (Kubernetes).::
There can be a number of reasons for this issue. Remember, though, that, for stage, we
assume that a sequence of manual steps needs to be performed. We do not
redeploy any existing services, because, most likely, you deliberately
have it set up that way. If, in the logs of your application,
you can see that you cannot connect to a service, first ensure that
the service is forwarding traffic to a pod. Next, if that is not the case,
delete the service and re-run the step in the pipeline. That way,
Spring Cloud Pipelines redeploy the service and the underlying pods.
